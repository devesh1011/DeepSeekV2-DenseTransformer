{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9mjZvOl-ap0",
        "outputId": "8a1b578e-27fa-44df-8116-f603c6b846a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, tiktoken, multiprocess, datasets\n",
            "Successfully installed datasets-3.4.1 dill-0.3.8 multiprocess-0.70.16 tiktoken-0.9.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f0Jz3H3t-fEj"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tiktoken\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pev-TLAz-qE7"
      },
      "outputs": [],
      "source": [
        "# class DataLoaderLite:\n",
        "#   def __init__(self, B, T):\n",
        "#     self.B = B\n",
        "#     self.T = T\n",
        "\n",
        "#     with open(\"input.txt\", \"r\") as f:\n",
        "#       text = f.read()\n",
        "#     enc = tiktoken.get_encoding(\"gpt2\")\n",
        "#     tokens = enc.encode(text)\n",
        "#     self.tokens = torch.tensor(tokens)\n",
        "\n",
        "#     print(f\"laoded {len(tokens)} tokens\")\n",
        "#     print(f\"1 Epoch = {len(self.tokens) // (B * T)} batches\")\n",
        "#     self.curr_pos = 0\n",
        "\n",
        "#   def next_batch(self):\n",
        "#     B, T = self.B, self.T\n",
        "#     buf = self.tokens[self.curr_pos : self.curr_pos + B * T + 1]\n",
        "\n",
        "#     x = buf[:-1].view(B, T)\n",
        "#     y = buf[1:].view(B, T)\n",
        "\n",
        "#     self.curr_pos += B * T\n",
        "#     if self.curr_pos + B * T >= len(self.tokens):\n",
        "#       self.curr_pos = 0\n",
        "#     return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VjeThVb2Jg0O"
      },
      "outputs": [],
      "source": [
        "class Dataloader:\n",
        "    def __init__(self, B, T, split=\"train\"):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "        self.split = split.lower()\n",
        "\n",
        "        # load pre tokenized data from binary files\n",
        "        if self.split == \"train\":\n",
        "            tokens_np = np.fromfile(\"train.bin\", dtype=np.uint16)\n",
        "            self.tokens = torch.tensor(tokens_np, dtype=torch.long)\n",
        "        elif self.split == \"val\":\n",
        "            tokens_np = np.fromfile(\"val.bin\", dtype=np.uint16)\n",
        "            self.tokens = torch.tensor(tokens_np, dtype=torch.long)\n",
        "        else:\n",
        "            raise ValueError(\"split must be 'train' or 'val'\")\n",
        "        self.curr_pos = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.curr_pos : self.curr_pos + B * T + 1]\n",
        "\n",
        "        # Handle case where remaining tokens are less than B * T + 1\n",
        "        if len(buf) < B * T + 1:\n",
        "            # Pad with zeros or cycle back (cycling is more common for language modeling)\n",
        "            pad_len = (B * T + 1) - len(buf)\n",
        "            buf = torch.cat([buf, self.tokens[:pad_len]])\n",
        "\n",
        "        x = buf[:-1].view(B, T)  # Input: all tokens except the last\n",
        "        y = buf[1:].view(B, T)  # Target: all tokens except the first\n",
        "\n",
        "        self.curr_pos += B * T\n",
        "        # Reset position if we've reached or exceeded the end\n",
        "        if self.curr_pos + B * T >= len(self.tokens):\n",
        "            self.curr_pos = 0\n",
        "\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7IaTffDG-rpv"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    block_size: int = 256\n",
        "    vocab_size: int = 50304\n",
        "    n_layers: int = 6\n",
        "    n_head: int = 6\n",
        "    n_embed: int = 384"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MnX_P8A4-twB"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNet(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embed, 4 * config.n_embed)\n",
        "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
        "        self.c_proj = nn.Linear(4 * config.n_embed, config.n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GMCCUxWu-vIy"
      },
      "outputs": [],
      "source": [
        "class MultiHeadLatentAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi Head Latent Attention (MLA) module for a transformer model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.d_kv = 128\n",
        "        self.d_c = 64\n",
        "        self.d_r = 64\n",
        "        self.d_q = 384\n",
        "        self.d_h = config.n_embed // config.n_head\n",
        "\n",
        "        self.norm_c_kv = nn.RMSNorm(self.d_kv)\n",
        "        self.norm_dq = nn.RMSNorm(self.d_q)\n",
        "        self.scale_kv = nn.Parameter(torch.ones(1))\n",
        "        self.scale_q = nn.Parameter(torch.ones(1))\n",
        "\n",
        "        # Shared projection matrices\n",
        "        self.W_dkv = nn.Linear(config.n_embed, self.d_kv, bias=False)\n",
        "        self.W_dq = nn.Linear(config.n_embed, self.d_q, bias=False)\n",
        "        self.W_kr = nn.Linear(config.n_embed, self.d_r, bias=False)\n",
        "\n",
        "        # Per-head projection matrices\n",
        "        self.W_qc = nn.ModuleList(\n",
        "            [nn.Linear(self.d_q, self.d_c, bias=False) for _ in range(config.n_head)]\n",
        "        )\n",
        "        self.W_qr = nn.ModuleList(\n",
        "            [nn.Linear(self.d_q, self.d_r, bias=False) for _ in range(config.n_head)]\n",
        "        )\n",
        "        self.W_uk = nn.ModuleList(\n",
        "            [nn.Linear(self.d_kv, self.d_c, bias=False) for _ in range(config.n_head)]\n",
        "        )\n",
        "        self.W_uv = nn.ModuleList(\n",
        "            [nn.Linear(self.d_kv, self.d_h, bias=False) for _ in range(config.n_head)]\n",
        "        )\n",
        "        # Output projection\n",
        "        self.W_o = nn.Linear(config.n_head * self.d_h, config.n_embed, bias=False)\n",
        "\n",
        "        # Precompute RoPE (Rotary Position Embedding) cosines and sines\n",
        "        freqs = torch.exp(\n",
        "            torch.arange(0, self.d_r // 2, dtype=torch.float)\n",
        "            * (-math.log(10000) / (self.d_r // 2))\n",
        "        )\n",
        "        positions = torch.arange(0, config.block_size)\n",
        "        angles = positions[:, None] * freqs[None, :]\n",
        "        self.register_buffer(\n",
        "            \"cos_angles\", torch.cos(angles)\n",
        "        )  # Shape: [block_size, d_r//2]\n",
        "        self.register_buffer(\"sin_angles\", torch.sin(angles))\n",
        "\n",
        "    def apply_rope(self, x, seq_len):\n",
        "        batch_size, _, d_r = x.size()\n",
        "        cos = self.cos_angles[:seq_len]\n",
        "        sin = self.sin_angles[:seq_len]\n",
        "        x_pairs = x.view(batch_size, seq_len, d_r // 2, 2)\n",
        "        rotated_x0 = (\n",
        "            x_pairs[..., 0] * cos[None, :, :] - x_pairs[..., 1] * sin[None, :, :]\n",
        "        )\n",
        "        rotated_x1 = (\n",
        "            x_pairs[..., 0] * sin[None, :, :] + x_pairs[..., 1] * cos[None, :, :]\n",
        "        )\n",
        "        rotated_pairs = torch.stack([rotated_x0, rotated_x1], dim=-1)\n",
        "        return rotated_pairs.view(batch_size, seq_len, d_r)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        c_kv = (\n",
        "            self.norm_c_kv(self.W_dkv(x)) * self.scale_kv\n",
        "        )  # [batch_size, seq_len, 128]\n",
        "        x_dq = self.norm_dq(self.W_dq(x)) * self.scale_q  # [batch_size, seq_len, 128]\n",
        "        k_rope = self.W_kr(x)  # [batch_size, seq_len, 64]\n",
        "        k_rope = self.apply_rope(k_rope, seq_len)\n",
        "\n",
        "        outputs = []\n",
        "        for h in range(self.config.n_head):\n",
        "            # Query projections\n",
        "            q_nope = self.W_qc[h](x_dq)  # [batch_size, seq_len, 64]\n",
        "            q_rope = self.W_qr[h](x_dq)  # [batch_size, seq_len, 64]\n",
        "            q_rope = self.apply_rope(q_rope, seq_len)\n",
        "\n",
        "            # Key and value projections from latent space\n",
        "            k_nope = self.W_uk[h](c_kv)  # [batch_size, seq_len, 64]\n",
        "            v = self.W_uv[h](c_kv)  # [batch_size, seq_len, 128]\n",
        "\n",
        "            # Compute attention scores\n",
        "            score_nope = torch.bmm(\n",
        "                q_nope, k_nope.transpose(1, 2)\n",
        "            )  # [batch_size, seq_len, seq_len]\n",
        "            score_rope = torch.bmm(\n",
        "                q_rope, k_rope.transpose(1, 2)\n",
        "            )  # [batch_size, seq_len, seq_len]\n",
        "            score = score_nope + score_rope\n",
        "\n",
        "            # Apply causal mask\n",
        "            mask = torch.triu(\n",
        "                torch.ones(seq_len, seq_len, device=x.device), diagonal=1\n",
        "            ).bool()\n",
        "            score = score.masked_fill(mask, float(\"-inf\"))\n",
        "\n",
        "            # Compute attention weights\n",
        "            attn_weights = F.softmax(\n",
        "                score / math.sqrt(self.d_h), dim=-1\n",
        "            )  # Scale by sqrt(d_h) = sqrt(128)\n",
        "\n",
        "            # Compute output for this head\n",
        "            out = torch.bmm(attn_weights, v)  # [batch_size, seq_len, 128]\n",
        "            outputs.append(out)\n",
        "\n",
        "        # Concatenate outputs from all heads\n",
        "        out = torch.cat(outputs, dim=-1)  # [batch_size, seq_len, 768]\n",
        "\n",
        "        # Final output projection\n",
        "        out = self.W_o(out)  # [batch_size, seq_len, 768]\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DDVvwPEY-wmz"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.rms_1 = nn.RMSNorm(config.n_embed)\n",
        "        self.attn = MultiHeadLatentAttention(config)\n",
        "        self.rms_2 = nn.RMSNorm(config.n_embed)\n",
        "        self.ffn = FeedForwardNet(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.rms_1(x))\n",
        "        x = x + self.ffn(self.rms_2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "n_9we9h0-x3S"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(\n",
        "            dict(\n",
        "                wte=nn.Embedding(config.vocab_size, config.n_embed),\n",
        "                h=nn.ModuleList(Block(config) for _ in range(config.n_layers)),\n",
        "                ln_f=nn.RMSNorm(config.n_embed),\n",
        "            )\n",
        "        )\n",
        "        self.ln_head = nn.Linear(config.n_embed, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing scheme\n",
        "        self.transformer.wte.weight = self.ln_head.weight\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size\n",
        "\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        x = tok_emb\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.ln_head(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "            return logits, loss\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "N0Tqqop7-0BU"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn4bDiWs-2ny",
        "outputId": "32800467-060c-4d23-bf62-ea2cf2076ca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total desired batch size: 8192\n",
            "=> Calculated gradient accumulation steps: 1\n"
          ]
        }
      ],
      "source": [
        "total_batch_size = 8192\n",
        "B = 32\n",
        "T = 256\n",
        "assert (\n",
        "    total_batch_size % (B * T) == 0\n",
        "), \"maken sure total_batch_size is divisible by B * T\"\n",
        "grad_accum_steps = total_batch_size // (B * T)\n",
        "print(f\"Total desired batch size: {total_batch_size}\")\n",
        "print(f\"=> Calculated gradient accumulation steps: {grad_accum_steps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "fOs0Y_8U-397"
      },
      "outputs": [],
      "source": [
        "# train_loader = DataLoaderLite(B=B, T=T)\n",
        "\n",
        "train_loader = Dataloader(B=B, T=T, split=\"train\")\n",
        "val_loader = Dataloader(B=B, T=T, split=\"val\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "NYLglev2Hjcm"
      },
      "outputs": [],
      "source": [
        "torch.set_float32_matmul_precision(\"high\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "tAhauUxS-5iF"
      },
      "outputs": [],
      "source": [
        "def get_lr(\n",
        "    it,\n",
        "    total_tokens=338025,\n",
        "    max_lr=2.4e-4,\n",
        "    warmup_steps=100,\n",
        "    decay_factors=[0.316, 0.316],\n",
        "    decay_points=[0.6, 0.9],\n",
        "):\n",
        "    # Convert decay points to token counts\n",
        "    decay_token_counts = [int(p * total_tokens) for p in decay_points]\n",
        "\n",
        "    # # Calculate total steps based on tokens\n",
        "    # total_steps = total_tokens // tokens_per_step\n",
        "\n",
        "    # # Adjust warmup_steps if it exceeds total_steps\n",
        "    # warmup_steps = min(warmup_steps, total_steps)\n",
        "\n",
        "    # # Calculate tokens processed at current iteration\n",
        "    # tokens_processed = it * tokens_per_step\n",
        "\n",
        "    if it < warmup_steps:\n",
        "        # Linear warmup from 0 to max_lr\n",
        "        lr = max_lr * (it / warmup_steps)\n",
        "    else:\n",
        "        # Base LR after warmup\n",
        "        lr = max_lr\n",
        "        # Apply step decay based on tokens processed\n",
        "        for decay_tokens, factor in zip(decay_token_counts, decay_factors):\n",
        "            if tokens_processed >= decay_tokens:\n",
        "                lr *= factor\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    return lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hHGqt_8y-8vr",
        "outputId": "b17835d2-74a2-4eab-aa1f-0a7de3b34e82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 11.0918\n",
            "step    0, loss: 11.072306 | lr: 0.0000e+00 | norm: 8.1408 dt: 4024.03ms, tok/sec: 2035.77\n",
            "step    1, loss: 11.037090 | lr: 2.4000e-06 | norm: 8.1998 dt: 525.11ms, tok/sec: 15600.46\n",
            "step    2, loss: 11.024903 | lr: 4.8000e-06 | norm: 8.7676 dt: 521.66ms, tok/sec: 15703.86\n",
            "step    3, loss: 10.969564 | lr: 7.2000e-06 | norm: 8.4936 dt: 530.97ms, tok/sec: 15428.24\n",
            "step    4, loss: 10.911911 | lr: 9.6000e-06 | norm: 7.9908 dt: 529.59ms, tok/sec: 15468.52\n",
            "step    5, loss: 10.811134 | lr: 1.2000e-05 | norm: 7.4857 dt: 530.67ms, tok/sec: 15437.02\n",
            "step    6, loss: 10.712528 | lr: 1.4400e-05 | norm: 7.1566 dt: 532.78ms, tok/sec: 15375.98\n",
            "step    7, loss: 10.574846 | lr: 1.6800e-05 | norm: 6.8427 dt: 532.24ms, tok/sec: 15391.64\n",
            "step    8, loss: 10.392406 | lr: 1.9200e-05 | norm: 6.6753 dt: 530.19ms, tok/sec: 15451.16\n",
            "step    9, loss: 10.322874 | lr: 2.1600e-05 | norm: 5.5015 dt: 533.59ms, tok/sec: 15352.72\n",
            "step   10, loss: 10.312893 | lr: 2.4000e-05 | norm: 5.0128 dt: 536.70ms, tok/sec: 15263.65\n",
            "step   11, loss: 10.160345 | lr: 2.6400e-05 | norm: 4.3798 dt: 537.73ms, tok/sec: 15234.39\n",
            "step   12, loss: 10.138449 | lr: 2.8800e-05 | norm: 3.8098 dt: 534.68ms, tok/sec: 15321.39\n",
            "step   13, loss: 10.006759 | lr: 3.1200e-05 | norm: 3.6746 dt: 535.14ms, tok/sec: 15308.02\n",
            "step   14, loss: 9.945258 | lr: 3.3600e-05 | norm: 3.3501 dt: 536.73ms, tok/sec: 15262.73\n",
            "step   15, loss: 9.837649 | lr: 3.6000e-05 | norm: 3.0853 dt: 536.80ms, tok/sec: 15260.69\n",
            "step   16, loss: 9.679379 | lr: 3.8400e-05 | norm: 3.0939 dt: 540.29ms, tok/sec: 15162.28\n",
            "step   17, loss: 9.659290 | lr: 4.0800e-05 | norm: 2.8123 dt: 538.31ms, tok/sec: 15218.11\n",
            "step   18, loss: 9.580799 | lr: 4.3200e-05 | norm: 2.7331 dt: 536.22ms, tok/sec: 15277.24\n",
            "step   19, loss: 9.544947 | lr: 4.5600e-05 | norm: 2.6016 dt: 540.88ms, tok/sec: 15145.80\n",
            "step   20, loss: 9.471203 | lr: 4.8000e-05 | norm: 2.4773 dt: 543.51ms, tok/sec: 15072.48\n",
            "step   21, loss: 9.382933 | lr: 5.0400e-05 | norm: 2.5137 dt: 541.06ms, tok/sec: 15140.68\n",
            "step   22, loss: 9.481897 | lr: 5.2800e-05 | norm: 2.2530 dt: 542.61ms, tok/sec: 15097.29\n",
            "step   23, loss: 9.328062 | lr: 5.5200e-05 | norm: 2.4395 dt: 544.79ms, tok/sec: 15037.11\n",
            "step   24, loss: 9.239417 | lr: 5.7600e-05 | norm: 2.4854 dt: 545.26ms, tok/sec: 15024.12\n",
            "step   25, loss: 9.281475 | lr: 6.0000e-05 | norm: 2.2392 dt: 542.73ms, tok/sec: 15093.93\n",
            "step   26, loss: 9.245390 | lr: 6.2400e-05 | norm: 2.3407 dt: 544.56ms, tok/sec: 15043.46\n",
            "step   27, loss: 9.152140 | lr: 6.4800e-05 | norm: 2.3518 dt: 546.16ms, tok/sec: 14999.36\n",
            "step   28, loss: 9.247042 | lr: 6.7200e-05 | norm: 2.0704 dt: 550.03ms, tok/sec: 14893.75\n",
            "step   29, loss: 9.088516 | lr: 6.9600e-05 | norm: 2.2111 dt: 543.91ms, tok/sec: 15061.21\n",
            "step   30, loss: 9.084342 | lr: 7.2000e-05 | norm: 2.0943 dt: 547.83ms, tok/sec: 14953.65\n",
            "step   31, loss: 8.963735 | lr: 7.4400e-05 | norm: 2.2398 dt: 597.95ms, tok/sec: 13700.20\n",
            "step   32, loss: 8.951861 | lr: 7.6800e-05 | norm: 2.2217 dt: 568.55ms, tok/sec: 14408.48\n",
            "step   33, loss: 9.017855 | lr: 7.9200e-05 | norm: 2.0720 dt: 584.22ms, tok/sec: 14022.01\n",
            "step   34, loss: 8.928866 | lr: 8.1600e-05 | norm: 2.0787 dt: 569.34ms, tok/sec: 14388.60\n",
            "step   35, loss: 8.883787 | lr: 8.4000e-05 | norm: 2.0028 dt: 541.63ms, tok/sec: 15124.83\n",
            "step   36, loss: 8.786542 | lr: 8.6400e-05 | norm: 2.1386 dt: 557.13ms, tok/sec: 14704.06\n",
            "step   37, loss: 8.808597 | lr: 8.8800e-05 | norm: 2.0570 dt: 543.08ms, tok/sec: 15084.25\n",
            "step   38, loss: 8.591965 | lr: 9.1200e-05 | norm: 2.2635 dt: 561.12ms, tok/sec: 14599.25\n",
            "step   39, loss: 8.546438 | lr: 9.3600e-05 | norm: 2.0785 dt: 551.17ms, tok/sec: 14862.82\n",
            "step   40, loss: 8.579945 | lr: 9.6000e-05 | norm: 1.8891 dt: 559.62ms, tok/sec: 14638.59\n",
            "step   41, loss: 8.595510 | lr: 9.8400e-05 | norm: 1.7263 dt: 553.28ms, tok/sec: 14806.30\n",
            "step   42, loss: 8.496369 | lr: 1.0080e-04 | norm: 1.8228 dt: 555.08ms, tok/sec: 14758.17\n",
            "step   43, loss: 8.376213 | lr: 1.0320e-04 | norm: 1.7575 dt: 552.90ms, tok/sec: 14816.46\n",
            "step   44, loss: 8.186754 | lr: 1.0560e-04 | norm: 1.7372 dt: 558.51ms, tok/sec: 14667.71\n",
            "step   45, loss: 8.257798 | lr: 1.0800e-04 | norm: 1.6063 dt: 558.98ms, tok/sec: 14655.28\n",
            "step   46, loss: 8.342884 | lr: 1.1040e-04 | norm: 1.5787 dt: 557.20ms, tok/sec: 14702.20\n",
            "step   47, loss: 8.278418 | lr: 1.1280e-04 | norm: 1.4874 dt: 553.25ms, tok/sec: 14807.02\n",
            "step   48, loss: 8.339376 | lr: 1.1520e-04 | norm: 1.3773 dt: 554.35ms, tok/sec: 14777.70\n",
            "step   49, loss: 8.225300 | lr: 1.1760e-04 | norm: 1.4456 dt: 555.70ms, tok/sec: 14741.65\n",
            "step   50, loss: 8.196259 | lr: 1.2000e-04 | norm: 1.4277 dt: 553.21ms, tok/sec: 14808.04\n",
            "step   51, loss: 8.087812 | lr: 1.2240e-04 | norm: 1.4494 dt: 553.60ms, tok/sec: 14797.56\n",
            "step   52, loss: 7.903352 | lr: 1.2480e-04 | norm: 1.5775 dt: 550.23ms, tok/sec: 14888.43\n",
            "step   53, loss: 7.924612 | lr: 1.2720e-04 | norm: 1.5524 dt: 551.66ms, tok/sec: 14849.83\n",
            "step   54, loss: 7.843460 | lr: 1.2960e-04 | norm: 1.5292 dt: 552.08ms, tok/sec: 14838.53\n",
            "step   55, loss: 7.749382 | lr: 1.3200e-04 | norm: 1.6071 dt: 548.16ms, tok/sec: 14944.46\n",
            "step   56, loss: 7.725910 | lr: 1.3440e-04 | norm: 1.4694 dt: 550.46ms, tok/sec: 14881.99\n",
            "step   57, loss: 7.590304 | lr: 1.3680e-04 | norm: 1.4116 dt: 545.37ms, tok/sec: 15020.96\n",
            "step   58, loss: 7.696320 | lr: 1.3920e-04 | norm: 1.2623 dt: 542.44ms, tok/sec: 15102.21\n",
            "step   59, loss: 7.496700 | lr: 1.4160e-04 | norm: 1.3759 dt: 545.56ms, tok/sec: 15015.84\n",
            "step   60, loss: 7.407940 | lr: 1.4400e-04 | norm: 1.3636 dt: 545.89ms, tok/sec: 15006.71\n",
            "step   61, loss: 7.457958 | lr: 1.4640e-04 | norm: 1.4631 dt: 544.05ms, tok/sec: 15057.40\n",
            "step   62, loss: 7.497785 | lr: 1.4880e-04 | norm: 1.4638 dt: 542.59ms, tok/sec: 15097.92\n",
            "step   63, loss: 7.361295 | lr: 1.5120e-04 | norm: 1.5322 dt: 542.50ms, tok/sec: 15100.56\n",
            "step   64, loss: 7.501162 | lr: 1.5360e-04 | norm: 1.3966 dt: 538.81ms, tok/sec: 15203.96\n",
            "step   65, loss: 7.255668 | lr: 1.5600e-04 | norm: 1.3102 dt: 534.12ms, tok/sec: 15337.31\n",
            "step   66, loss: 7.249582 | lr: 1.5840e-04 | norm: 1.2235 dt: 539.24ms, tok/sec: 15191.77\n",
            "step   67, loss: 7.101705 | lr: 1.6080e-04 | norm: 1.4207 dt: 534.05ms, tok/sec: 15339.26\n",
            "step   68, loss: 7.032991 | lr: 1.6320e-04 | norm: 1.5027 dt: 535.50ms, tok/sec: 15297.97\n",
            "step   69, loss: 7.164122 | lr: 1.6560e-04 | norm: 1.2466 dt: 533.05ms, tok/sec: 15368.09\n",
            "step   70, loss: 7.020995 | lr: 1.6800e-04 | norm: 1.2177 dt: 535.13ms, tok/sec: 15308.37\n",
            "step   71, loss: 6.956452 | lr: 1.7040e-04 | norm: 1.1090 dt: 534.56ms, tok/sec: 15324.69\n",
            "step   72, loss: 6.916780 | lr: 1.7280e-04 | norm: 1.1138 dt: 532.36ms, tok/sec: 15388.14\n",
            "step   73, loss: 6.939104 | lr: 1.7520e-04 | norm: 1.2472 dt: 534.29ms, tok/sec: 15332.43\n",
            "step   74, loss: 6.676228 | lr: 1.7760e-04 | norm: 1.3415 dt: 532.49ms, tok/sec: 15384.24\n",
            "step   75, loss: 6.616107 | lr: 1.8000e-04 | norm: 1.1093 dt: 531.57ms, tok/sec: 15410.84\n",
            "step   76, loss: 6.648276 | lr: 1.8240e-04 | norm: 1.0490 dt: 535.71ms, tok/sec: 15291.90\n",
            "step   77, loss: 6.711421 | lr: 1.8480e-04 | norm: 0.9354 dt: 528.07ms, tok/sec: 15513.19\n",
            "step   78, loss: 6.628716 | lr: 1.8720e-04 | norm: 1.1872 dt: 535.01ms, tok/sec: 15311.77\n",
            "step   79, loss: 6.496295 | lr: 1.8960e-04 | norm: 1.0139 dt: 531.44ms, tok/sec: 15414.64\n",
            "step   80, loss: 6.311380 | lr: 1.9200e-04 | norm: 1.0164 dt: 532.37ms, tok/sec: 15387.73\n",
            "step   81, loss: 6.454233 | lr: 1.9440e-04 | norm: 0.9039 dt: 531.95ms, tok/sec: 15400.08\n",
            "step   82, loss: 6.542857 | lr: 1.9680e-04 | norm: 1.2251 dt: 534.39ms, tok/sec: 15329.65\n",
            "step   83, loss: 6.545503 | lr: 1.9920e-04 | norm: 1.2108 dt: 529.22ms, tok/sec: 15479.37\n",
            "step   84, loss: 6.617251 | lr: 2.0160e-04 | norm: 0.9210 dt: 534.48ms, tok/sec: 15327.09\n",
            "step   85, loss: 6.499847 | lr: 2.0400e-04 | norm: 0.8919 dt: 529.87ms, tok/sec: 15460.33\n",
            "step   86, loss: 6.485013 | lr: 2.0640e-04 | norm: 0.8933 dt: 530.65ms, tok/sec: 15437.74\n",
            "step   87, loss: 6.360102 | lr: 2.0880e-04 | norm: 0.9522 dt: 527.07ms, tok/sec: 15542.62\n",
            "step   88, loss: 6.262923 | lr: 2.1120e-04 | norm: 1.5195 dt: 529.39ms, tok/sec: 15474.33\n",
            "step   89, loss: 6.308383 | lr: 2.1360e-04 | norm: 0.7189 dt: 525.58ms, tok/sec: 15586.68\n",
            "step   90, loss: 6.239119 | lr: 2.1600e-04 | norm: 0.7806 dt: 530.20ms, tok/sec: 15450.86\n",
            "step   91, loss: 6.118551 | lr: 2.1840e-04 | norm: 0.8149 dt: 531.41ms, tok/sec: 15415.48\n",
            "step   92, loss: 6.187605 | lr: 2.2080e-04 | norm: 0.7299 dt: 527.27ms, tok/sec: 15536.62\n",
            "step   93, loss: 6.041574 | lr: 2.2320e-04 | norm: 0.7140 dt: 526.19ms, tok/sec: 15568.67\n",
            "step   94, loss: 6.206334 | lr: 2.2560e-04 | norm: 0.7012 dt: 522.78ms, tok/sec: 15670.18\n",
            "step   95, loss: 6.013385 | lr: 2.2800e-04 | norm: 0.9726 dt: 529.18ms, tok/sec: 15480.52\n",
            "step   96, loss: 5.967171 | lr: 2.3040e-04 | norm: 0.7604 dt: 529.13ms, tok/sec: 15482.09\n",
            "step   97, loss: 6.072035 | lr: 2.3280e-04 | norm: 1.0009 dt: 528.97ms, tok/sec: 15486.69\n",
            "step   98, loss: 6.211143 | lr: 2.3520e-04 | norm: 0.9475 dt: 526.74ms, tok/sec: 15552.16\n",
            "step   99, loss: 6.101905 | lr: 2.3760e-04 | norm: 0.9680 dt: 529.27ms, tok/sec: 15478.05\n",
            "Validation loss: 6.1579\n",
            "step  100, loss: 6.314256 | lr: 2.4000e-04 | norm: 1.0439 dt: 3980.54ms, tok/sec: 2058.01\n",
            "step  101, loss: 6.059620 | lr: 2.4000e-04 | norm: 0.7745 dt: 517.18ms, tok/sec: 15839.81\n",
            "step  102, loss: 6.088901 | lr: 2.4000e-04 | norm: 0.7910 dt: 530.28ms, tok/sec: 15448.38\n",
            "step  103, loss: 5.971934 | lr: 2.4000e-04 | norm: 0.9057 dt: 522.05ms, tok/sec: 15692.12\n",
            "step  104, loss: 5.879413 | lr: 2.4000e-04 | norm: 0.9168 dt: 529.48ms, tok/sec: 15471.92\n",
            "step  105, loss: 6.133881 | lr: 2.4000e-04 | norm: 1.1925 dt: 523.94ms, tok/sec: 15635.46\n",
            "step  106, loss: 5.992087 | lr: 2.4000e-04 | norm: 0.9790 dt: 525.50ms, tok/sec: 15589.03\n",
            "step  107, loss: 5.970305 | lr: 2.4000e-04 | norm: 0.7229 dt: 522.15ms, tok/sec: 15688.93\n",
            "step  108, loss: 5.998911 | lr: 2.4000e-04 | norm: 0.8793 dt: 523.95ms, tok/sec: 15635.06\n",
            "step  109, loss: 6.008963 | lr: 2.4000e-04 | norm: 0.8121 dt: 525.54ms, tok/sec: 15587.70\n",
            "step  110, loss: 5.735372 | lr: 2.4000e-04 | norm: 1.0395 dt: 522.95ms, tok/sec: 15665.07\n",
            "step  111, loss: 5.708739 | lr: 2.4000e-04 | norm: 0.9930 dt: 520.65ms, tok/sec: 15734.20\n",
            "step  112, loss: 5.746368 | lr: 2.4000e-04 | norm: 0.8457 dt: 527.64ms, tok/sec: 15525.77\n",
            "step  113, loss: 5.897109 | lr: 2.4000e-04 | norm: 0.7071 dt: 523.72ms, tok/sec: 15641.88\n",
            "step  114, loss: 5.851326 | lr: 2.4000e-04 | norm: 0.9917 dt: 521.76ms, tok/sec: 15700.65\n",
            "step  115, loss: 5.751174 | lr: 2.4000e-04 | norm: 0.9424 dt: 527.55ms, tok/sec: 15528.39\n",
            "step  116, loss: 5.585356 | lr: 2.4000e-04 | norm: 0.9480 dt: 524.30ms, tok/sec: 15624.69\n",
            "step  117, loss: 5.790399 | lr: 2.4000e-04 | norm: 0.8610 dt: 528.48ms, tok/sec: 15501.06\n",
            "step  118, loss: 5.845411 | lr: 2.4000e-04 | norm: 1.0316 dt: 525.62ms, tok/sec: 15585.52\n",
            "step  119, loss: 5.989038 | lr: 2.4000e-04 | norm: 0.8090 dt: 527.96ms, tok/sec: 15516.43\n",
            "step  120, loss: 6.080860 | lr: 2.4000e-04 | norm: 0.8043 dt: 524.73ms, tok/sec: 15611.81\n",
            "step  121, loss: 5.968145 | lr: 2.4000e-04 | norm: 0.9333 dt: 522.39ms, tok/sec: 15681.81\n",
            "step  122, loss: 5.966337 | lr: 2.4000e-04 | norm: 0.6199 dt: 527.40ms, tok/sec: 15532.80\n",
            "step  123, loss: 5.805108 | lr: 2.4000e-04 | norm: 0.8960 dt: 526.73ms, tok/sec: 15552.70\n",
            "step  124, loss: 5.774387 | lr: 2.4000e-04 | norm: 0.7626 dt: 526.91ms, tok/sec: 15547.34\n",
            "step  125, loss: 5.839579 | lr: 2.4000e-04 | norm: 0.6822 dt: 527.45ms, tok/sec: 15531.45\n",
            "step  126, loss: 5.767125 | lr: 2.4000e-04 | norm: 0.7560 dt: 528.88ms, tok/sec: 15489.34\n",
            "step  127, loss: 5.649579 | lr: 2.4000e-04 | norm: 0.7633 dt: 527.85ms, tok/sec: 15519.63\n",
            "step  128, loss: 5.776871 | lr: 2.4000e-04 | norm: 0.6782 dt: 527.83ms, tok/sec: 15520.03\n",
            "step  129, loss: 5.610740 | lr: 2.4000e-04 | norm: 0.7208 dt: 525.73ms, tok/sec: 15582.02\n",
            "step  130, loss: 5.827389 | lr: 2.4000e-04 | norm: 0.8331 dt: 525.66ms, tok/sec: 15584.25\n",
            "step  131, loss: 5.584583 | lr: 2.4000e-04 | norm: 0.7400 dt: 524.80ms, tok/sec: 15609.81\n",
            "step  132, loss: 5.556871 | lr: 2.4000e-04 | norm: 0.8132 dt: 528.66ms, tok/sec: 15495.82\n",
            "step  133, loss: 5.668743 | lr: 2.4000e-04 | norm: 0.7229 dt: 525.06ms, tok/sec: 15602.14\n",
            "step  134, loss: 5.847153 | lr: 2.4000e-04 | norm: 0.9999 dt: 528.51ms, tok/sec: 15500.19\n",
            "step  135, loss: 5.764855 | lr: 2.4000e-04 | norm: 0.9574 dt: 524.03ms, tok/sec: 15632.84\n",
            "step  136, loss: 6.015532 | lr: 2.4000e-04 | norm: 0.9653 dt: 527.97ms, tok/sec: 15515.92\n",
            "step  137, loss: 5.730530 | lr: 2.4000e-04 | norm: 0.8266 dt: 526.92ms, tok/sec: 15546.84\n",
            "step  138, loss: 5.759273 | lr: 2.4000e-04 | norm: 0.8718 dt: 528.65ms, tok/sec: 15495.99\n",
            "step  139, loss: 5.603982 | lr: 2.4000e-04 | norm: 1.0490 dt: 529.29ms, tok/sec: 15477.30\n",
            "step  140, loss: 5.514233 | lr: 2.4000e-04 | norm: 0.9264 dt: 529.76ms, tok/sec: 15463.65\n",
            "step  141, loss: 5.749462 | lr: 2.4000e-04 | norm: 1.0196 dt: 530.42ms, tok/sec: 15444.36\n",
            "step  142, loss: 5.595685 | lr: 2.4000e-04 | norm: 0.8922 dt: 529.77ms, tok/sec: 15463.33\n",
            "step  143, loss: 5.646147 | lr: 2.4000e-04 | norm: 0.8877 dt: 529.40ms, tok/sec: 15474.25\n",
            "step  144, loss: 5.662770 | lr: 2.4000e-04 | norm: 0.8552 dt: 528.68ms, tok/sec: 15495.33\n",
            "step  145, loss: 5.671447 | lr: 2.4000e-04 | norm: 0.8693 dt: 530.49ms, tok/sec: 15442.38\n",
            "step  146, loss: 5.353799 | lr: 2.4000e-04 | norm: 1.0698 dt: 532.41ms, tok/sec: 15386.58\n",
            "step  147, loss: 5.349411 | lr: 2.4000e-04 | norm: 1.0388 dt: 531.64ms, tok/sec: 15408.98\n",
            "step  148, loss: 5.415981 | lr: 2.4000e-04 | norm: 0.9573 dt: 531.25ms, tok/sec: 15420.16\n",
            "step  149, loss: 5.571077 | lr: 2.4000e-04 | norm: 0.6936 dt: 534.47ms, tok/sec: 15327.26\n",
            "step  150, loss: 5.501557 | lr: 2.4000e-04 | norm: 1.0770 dt: 526.37ms, tok/sec: 15563.13\n",
            "step  151, loss: 5.428100 | lr: 2.4000e-04 | norm: 0.7168 dt: 533.24ms, tok/sec: 15362.76\n",
            "step  152, loss: 5.246316 | lr: 2.4000e-04 | norm: 0.9878 dt: 529.85ms, tok/sec: 15460.96\n",
            "step  153, loss: 5.462842 | lr: 2.4000e-04 | norm: 0.7986 dt: 535.47ms, tok/sec: 15298.80\n",
            "step  154, loss: 5.483293 | lr: 2.4000e-04 | norm: 1.0207 dt: 529.85ms, tok/sec: 15461.09\n",
            "step  155, loss: 5.690387 | lr: 2.4000e-04 | norm: 0.8016 dt: 530.75ms, tok/sec: 15434.64\n",
            "step  156, loss: 5.795611 | lr: 2.4000e-04 | norm: 0.7214 dt: 533.30ms, tok/sec: 15360.96\n",
            "step  157, loss: 5.670133 | lr: 2.4000e-04 | norm: 0.8189 dt: 534.96ms, tok/sec: 15313.34\n",
            "step  158, loss: 5.664616 | lr: 2.4000e-04 | norm: 0.8190 dt: 532.05ms, tok/sec: 15397.01\n",
            "step  159, loss: 5.458558 | lr: 2.4000e-04 | norm: 1.0075 dt: 538.00ms, tok/sec: 15226.73\n",
            "step  160, loss: 5.469944 | lr: 2.4000e-04 | norm: 0.7642 dt: 533.31ms, tok/sec: 15360.61\n",
            "step  161, loss: 5.552630 | lr: 2.4000e-04 | norm: 0.6514 dt: 535.92ms, tok/sec: 15285.88\n",
            "step  162, loss: 5.499094 | lr: 2.4000e-04 | norm: 0.7443 dt: 532.81ms, tok/sec: 15375.03\n",
            "step  163, loss: 5.363200 | lr: 2.4000e-04 | norm: 0.9315 dt: 539.09ms, tok/sec: 15195.85\n",
            "step  164, loss: 5.496033 | lr: 2.4000e-04 | norm: 0.7157 dt: 534.87ms, tok/sec: 15315.77\n",
            "step  165, loss: 5.326978 | lr: 2.4000e-04 | norm: 0.8791 dt: 537.31ms, tok/sec: 15246.41\n",
            "step  166, loss: 5.562545 | lr: 2.4000e-04 | norm: 0.7952 dt: 537.75ms, tok/sec: 15233.74\n",
            "step  167, loss: 5.305144 | lr: 2.4000e-04 | norm: 0.8169 dt: 542.01ms, tok/sec: 15113.98\n",
            "step  168, loss: 5.283944 | lr: 2.4000e-04 | norm: 0.8629 dt: 535.07ms, tok/sec: 15310.05\n",
            "step  169, loss: 5.376623 | lr: 2.4000e-04 | norm: 0.7906 dt: 540.70ms, tok/sec: 15150.69\n",
            "step  170, loss: 5.597383 | lr: 2.4000e-04 | norm: 1.2208 dt: 537.84ms, tok/sec: 15231.22\n",
            "step  171, loss: 5.526693 | lr: 2.4000e-04 | norm: 1.0948 dt: 538.88ms, tok/sec: 15201.95\n",
            "step  172, loss: 5.816304 | lr: 2.4000e-04 | norm: 1.3033 dt: 533.99ms, tok/sec: 15341.05\n",
            "step  173, loss: 5.504857 | lr: 2.4000e-04 | norm: 1.0797 dt: 538.09ms, tok/sec: 15224.10\n",
            "step  174, loss: 5.520163 | lr: 2.4000e-04 | norm: 0.8082 dt: 537.04ms, tok/sec: 15254.03\n",
            "step  175, loss: 5.325033 | lr: 2.4000e-04 | norm: 1.0719 dt: 538.52ms, tok/sec: 15212.01\n",
            "step  176, loss: 5.214256 | lr: 2.4000e-04 | norm: 1.0443 dt: 537.33ms, tok/sec: 15245.84\n",
            "step  177, loss: 5.436256 | lr: 2.4000e-04 | norm: 1.1520 dt: 539.25ms, tok/sec: 15191.48\n",
            "step  178, loss: 5.269610 | lr: 2.4000e-04 | norm: 1.0554 dt: 534.66ms, tok/sec: 15322.01\n",
            "step  179, loss: 5.363286 | lr: 2.4000e-04 | norm: 0.7715 dt: 542.31ms, tok/sec: 15105.76\n",
            "step  180, loss: 5.396843 | lr: 2.4000e-04 | norm: 0.8329 dt: 536.87ms, tok/sec: 15258.71\n",
            "step  181, loss: 5.412134 | lr: 2.4000e-04 | norm: 0.9117 dt: 542.20ms, tok/sec: 15108.76\n",
            "step  182, loss: 5.070844 | lr: 2.4000e-04 | norm: 0.8832 dt: 537.20ms, tok/sec: 15249.40\n",
            "step  183, loss: 5.067111 | lr: 2.4000e-04 | norm: 0.8367 dt: 542.30ms, tok/sec: 15105.95\n",
            "step  184, loss: 5.139584 | lr: 2.4000e-04 | norm: 0.9590 dt: 541.35ms, tok/sec: 15132.44\n",
            "step  185, loss: 5.304688 | lr: 2.4000e-04 | norm: 0.7445 dt: 540.21ms, tok/sec: 15164.49\n",
            "step  186, loss: 5.200699 | lr: 2.4000e-04 | norm: 1.0660 dt: 540.95ms, tok/sec: 15143.83\n",
            "step  187, loss: 5.173785 | lr: 2.4000e-04 | norm: 0.8344 dt: 543.88ms, tok/sec: 15062.04\n",
            "step  188, loss: 4.945409 | lr: 2.4000e-04 | norm: 0.8342 dt: 544.98ms, tok/sec: 15031.79\n",
            "step  189, loss: 5.174162 | lr: 2.4000e-04 | norm: 0.9332 dt: 543.58ms, tok/sec: 15070.35\n",
            "step  190, loss: 5.155848 | lr: 2.4000e-04 | norm: 1.0543 dt: 545.06ms, tok/sec: 15029.63\n",
            "step  191, loss: 5.431426 | lr: 2.4000e-04 | norm: 0.8493 dt: 536.72ms, tok/sec: 15263.12\n",
            "step  192, loss: 5.543064 | lr: 2.4000e-04 | norm: 0.8286 dt: 540.12ms, tok/sec: 15166.99\n",
            "step  193, loss: 5.402892 | lr: 2.4000e-04 | norm: 0.8355 dt: 540.77ms, tok/sec: 15148.75\n",
            "step  194, loss: 5.389798 | lr: 2.4000e-04 | norm: 0.7935 dt: 542.42ms, tok/sec: 15102.73\n",
            "step  195, loss: 5.148896 | lr: 2.4000e-04 | norm: 0.9116 dt: 542.70ms, tok/sec: 15094.77\n",
            "step  196, loss: 5.221924 | lr: 2.4000e-04 | norm: 0.8050 dt: 544.57ms, tok/sec: 15042.98\n",
            "step  197, loss: 5.317387 | lr: 2.4000e-04 | norm: 0.7493 dt: 540.35ms, tok/sec: 15160.41\n",
            "step  198, loss: 5.263000 | lr: 2.4000e-04 | norm: 0.7881 dt: 540.42ms, tok/sec: 15158.65\n",
            "step  199, loss: 5.120761 | lr: 2.4000e-04 | norm: 0.9505 dt: 541.76ms, tok/sec: 15121.14\n",
            "Validation loss: 5.5684\n",
            "step  200, loss: 5.275061 | lr: 2.4000e-04 | norm: 0.8492 dt: 4136.23ms, tok/sec: 1980.55\n",
            "step  201, loss: 5.080452 | lr: 2.4000e-04 | norm: 0.9089 dt: 541.70ms, tok/sec: 15122.76\n",
            "step  202, loss: 5.326323 | lr: 2.4000e-04 | norm: 0.8809 dt: 543.88ms, tok/sec: 15062.09\n",
            "step  203, loss: 5.045727 | lr: 2.4000e-04 | norm: 0.8647 dt: 538.31ms, tok/sec: 15218.04\n",
            "step  204, loss: 5.045525 | lr: 2.4000e-04 | norm: 0.8607 dt: 545.71ms, tok/sec: 15011.54\n",
            "step  205, loss: 5.120564 | lr: 2.4000e-04 | norm: 0.8821 dt: 537.39ms, tok/sec: 15244.07\n",
            "step  206, loss: 5.381240 | lr: 2.4000e-04 | norm: 1.1688 dt: 540.82ms, tok/sec: 15147.49\n",
            "step  207, loss: 5.316909 | lr: 2.4000e-04 | norm: 1.0558 dt: 539.21ms, tok/sec: 15192.55\n",
            "step  208, loss: 5.630897 | lr: 2.4000e-04 | norm: 1.1329 dt: 544.70ms, tok/sec: 15039.57\n",
            "step  209, loss: 5.304438 | lr: 2.4000e-04 | norm: 1.3050 dt: 539.05ms, tok/sec: 15197.20\n",
            "step  210, loss: 5.324365 | lr: 2.4000e-04 | norm: 0.9030 dt: 543.25ms, tok/sec: 15079.75\n",
            "step  211, loss: 5.109193 | lr: 2.4000e-04 | norm: 1.1682 dt: 539.45ms, tok/sec: 15185.97\n",
            "step  212, loss: 4.994762 | lr: 2.4000e-04 | norm: 1.2693 dt: 541.92ms, tok/sec: 15116.71\n",
            "step  213, loss: 5.208933 | lr: 2.4000e-04 | norm: 1.5105 dt: 540.99ms, tok/sec: 15142.62\n",
            "step  214, loss: 5.015773 | lr: 2.4000e-04 | norm: 1.1302 dt: 545.36ms, tok/sec: 15021.16\n",
            "step  215, loss: 5.150207 | lr: 2.4000e-04 | norm: 1.0543 dt: 541.98ms, tok/sec: 15115.03\n",
            "step  216, loss: 5.168534 | lr: 2.4000e-04 | norm: 0.8204 dt: 539.99ms, tok/sec: 15170.57\n",
            "step  217, loss: 5.190937 | lr: 2.4000e-04 | norm: 0.9394 dt: 537.87ms, tok/sec: 15230.31\n",
            "step  218, loss: 4.842220 | lr: 2.4000e-04 | norm: 1.0126 dt: 545.11ms, tok/sec: 15028.08\n",
            "step  219, loss: 4.829051 | lr: 2.4000e-04 | norm: 0.7733 dt: 537.20ms, tok/sec: 15249.40\n",
            "step  220, loss: 4.914986 | lr: 2.4000e-04 | norm: 0.8706 dt: 538.33ms, tok/sec: 15217.46\n",
            "step  221, loss: 5.089485 | lr: 2.4000e-04 | norm: 0.8014 dt: 536.60ms, tok/sec: 15266.42\n",
            "step  222, loss: 4.954628 | lr: 2.4000e-04 | norm: 1.1391 dt: 543.66ms, tok/sec: 15068.21\n",
            "step  223, loss: 4.979968 | lr: 2.4000e-04 | norm: 0.9986 dt: 540.93ms, tok/sec: 15144.18\n",
            "step  224, loss: 4.725770 | lr: 2.4000e-04 | norm: 0.9561 dt: 541.13ms, tok/sec: 15138.79\n",
            "step  225, loss: 4.947004 | lr: 2.4000e-04 | norm: 1.0020 dt: 534.89ms, tok/sec: 15315.19\n",
            "step  226, loss: 4.885895 | lr: 2.4000e-04 | norm: 1.1268 dt: 538.83ms, tok/sec: 15203.29\n",
            "step  227, loss: 5.212165 | lr: 2.4000e-04 | norm: 0.8310 dt: 532.63ms, tok/sec: 15380.16\n",
            "step  228, loss: 5.318191 | lr: 2.4000e-04 | norm: 0.8350 dt: 540.15ms, tok/sec: 15166.17\n",
            "step  229, loss: 5.178202 | lr: 2.4000e-04 | norm: 0.9820 dt: 535.12ms, tok/sec: 15308.85\n",
            "step  230, loss: 5.162788 | lr: 2.4000e-04 | norm: 0.8265 dt: 536.86ms, tok/sec: 15259.03\n",
            "step  231, loss: 4.902849 | lr: 2.4000e-04 | norm: 0.9070 dt: 535.66ms, tok/sec: 15293.33\n",
            "step  232, loss: 5.045343 | lr: 2.4000e-04 | norm: 1.0558 dt: 540.24ms, tok/sec: 15163.64\n",
            "step  233, loss: 5.126586 | lr: 2.4000e-04 | norm: 0.8409 dt: 534.96ms, tok/sec: 15313.24\n",
            "step  234, loss: 5.069446 | lr: 2.4000e-04 | norm: 0.8627 dt: 539.96ms, tok/sec: 15171.37\n",
            "step  235, loss: 4.933207 | lr: 2.4000e-04 | norm: 0.9145 dt: 537.17ms, tok/sec: 15250.40\n",
            "step  236, loss: 5.093349 | lr: 2.4000e-04 | norm: 0.9679 dt: 535.87ms, tok/sec: 15287.36\n",
            "step  237, loss: 4.895441 | lr: 2.4000e-04 | norm: 1.1411 dt: 534.79ms, tok/sec: 15318.16\n",
            "step  238, loss: 5.128191 | lr: 2.4000e-04 | norm: 0.7914 dt: 535.57ms, tok/sec: 15295.88\n",
            "step  239, loss: 4.856196 | lr: 2.4000e-04 | norm: 1.0826 dt: 537.08ms, tok/sec: 15252.83\n",
            "step  240, loss: 4.853241 | lr: 2.4000e-04 | norm: 0.9865 dt: 538.71ms, tok/sec: 15206.82\n",
            "step  241, loss: 4.905679 | lr: 2.4000e-04 | norm: 0.9325 dt: 538.05ms, tok/sec: 15225.49\n",
            "step  242, loss: 5.215746 | lr: 2.4000e-04 | norm: 1.1717 dt: 537.08ms, tok/sec: 15252.84\n",
            "step  243, loss: 5.145076 | lr: 2.4000e-04 | norm: 1.0102 dt: 538.05ms, tok/sec: 15225.30\n",
            "step  244, loss: 5.462283 | lr: 2.4000e-04 | norm: 0.9630 dt: 534.65ms, tok/sec: 15322.21\n",
            "step  245, loss: 5.134501 | lr: 2.4000e-04 | norm: 1.1538 dt: 537.40ms, tok/sec: 15243.87\n",
            "step  246, loss: 5.149080 | lr: 2.4000e-04 | norm: 0.9399 dt: 533.19ms, tok/sec: 15364.04\n",
            "step  247, loss: 4.914445 | lr: 2.4000e-04 | norm: 0.9142 dt: 534.32ms, tok/sec: 15331.59\n",
            "step  248, loss: 4.830962 | lr: 2.4000e-04 | norm: 1.4004 dt: 536.47ms, tok/sec: 15270.25\n",
            "step  249, loss: 5.031118 | lr: 2.4000e-04 | norm: 1.5357 dt: 536.15ms, tok/sec: 15279.28\n",
            "step  250, loss: 4.823864 | lr: 2.4000e-04 | norm: 1.2024 dt: 535.39ms, tok/sec: 15301.11\n",
            "step  251, loss: 4.962646 | lr: 2.4000e-04 | norm: 0.9247 dt: 534.31ms, tok/sec: 15331.86\n",
            "step  252, loss: 4.997545 | lr: 2.4000e-04 | norm: 1.0132 dt: 533.75ms, tok/sec: 15347.92\n",
            "step  253, loss: 5.005039 | lr: 2.4000e-04 | norm: 1.0452 dt: 536.84ms, tok/sec: 15259.70\n",
            "step  254, loss: 4.653180 | lr: 2.4000e-04 | norm: 0.9164 dt: 532.70ms, tok/sec: 15378.30\n",
            "step  255, loss: 4.661172 | lr: 2.4000e-04 | norm: 1.0609 dt: 536.78ms, tok/sec: 15261.27\n",
            "step  256, loss: 4.761353 | lr: 2.4000e-04 | norm: 1.1379 dt: 530.89ms, tok/sec: 15430.81\n",
            "step  257, loss: 4.913048 | lr: 2.4000e-04 | norm: 0.8101 dt: 534.53ms, tok/sec: 15325.58\n",
            "step  258, loss: 4.736526 | lr: 2.4000e-04 | norm: 1.0974 dt: 535.95ms, tok/sec: 15285.04\n",
            "step  259, loss: 4.791136 | lr: 2.4000e-04 | norm: 1.2264 dt: 535.63ms, tok/sec: 15294.27\n",
            "step  260, loss: 4.542826 | lr: 2.4000e-04 | norm: 1.1517 dt: 535.18ms, tok/sec: 15307.11\n",
            "step  261, loss: 4.749379 | lr: 2.4000e-04 | norm: 0.7914 dt: 534.01ms, tok/sec: 15340.55\n",
            "step  262, loss: 4.678675 | lr: 2.4000e-04 | norm: 1.2933 dt: 536.77ms, tok/sec: 15261.52\n",
            "step  263, loss: 5.038964 | lr: 2.4000e-04 | norm: 1.1151 dt: 530.33ms, tok/sec: 15447.13\n",
            "step  264, loss: 5.135051 | lr: 2.4000e-04 | norm: 0.9641 dt: 536.07ms, tok/sec: 15281.66\n",
            "step  265, loss: 4.985078 | lr: 2.4000e-04 | norm: 0.8566 dt: 532.49ms, tok/sec: 15384.38\n",
            "step  266, loss: 4.976034 | lr: 2.4000e-04 | norm: 1.0695 dt: 530.47ms, tok/sec: 15443.04\n",
            "step  267, loss: 4.699083 | lr: 2.4000e-04 | norm: 0.9236 dt: 533.68ms, tok/sec: 15350.05\n",
            "step  268, loss: 4.869252 | lr: 2.4000e-04 | norm: 0.8278 dt: 538.15ms, tok/sec: 15222.47\n",
            "step  269, loss: 4.971859 | lr: 2.4000e-04 | norm: 0.9892 dt: 532.72ms, tok/sec: 15377.80\n",
            "step  270, loss: 4.902826 | lr: 2.4000e-04 | norm: 0.9169 dt: 538.72ms, tok/sec: 15206.45\n",
            "step  271, loss: 4.755674 | lr: 2.4000e-04 | norm: 0.8944 dt: 532.04ms, tok/sec: 15397.35\n",
            "step  272, loss: 4.921825 | lr: 2.4000e-04 | norm: 0.8678 dt: 535.73ms, tok/sec: 15291.40\n",
            "step  273, loss: 4.725258 | lr: 2.4000e-04 | norm: 1.0427 dt: 530.79ms, tok/sec: 15433.55\n",
            "step  274, loss: 4.958686 | lr: 2.4000e-04 | norm: 0.8665 dt: 538.58ms, tok/sec: 15210.23\n",
            "step  275, loss: 4.666596 | lr: 2.4000e-04 | norm: 0.9577 dt: 536.00ms, tok/sec: 15283.47\n",
            "step  276, loss: 4.678424 | lr: 2.4000e-04 | norm: 1.0205 dt: 534.53ms, tok/sec: 15325.63\n",
            "step  277, loss: 4.725098 | lr: 2.4000e-04 | norm: 1.0670 dt: 536.61ms, tok/sec: 15266.30\n",
            "step  278, loss: 5.073295 | lr: 2.4000e-04 | norm: 1.3466 dt: 536.24ms, tok/sec: 15276.84\n",
            "step  279, loss: 4.982722 | lr: 2.4000e-04 | norm: 1.0026 dt: 534.83ms, tok/sec: 15317.13\n",
            "step  280, loss: 5.307232 | lr: 2.4000e-04 | norm: 1.0904 dt: 533.42ms, tok/sec: 15357.54\n",
            "step  281, loss: 4.980120 | lr: 2.4000e-04 | norm: 1.1104 dt: 532.97ms, tok/sec: 15370.53\n",
            "step  282, loss: 5.022801 | lr: 2.4000e-04 | norm: 1.2695 dt: 537.28ms, tok/sec: 15247.18\n",
            "step  283, loss: 4.759087 | lr: 2.4000e-04 | norm: 1.2010 dt: 537.49ms, tok/sec: 15241.22\n",
            "step  284, loss: 4.673933 | lr: 2.4000e-04 | norm: 1.3008 dt: 535.10ms, tok/sec: 15309.20\n",
            "step  285, loss: 4.800161 | lr: 2.4000e-04 | norm: 1.0186 dt: 534.16ms, tok/sec: 15336.17\n",
            "step  286, loss: 4.619236 | lr: 2.4000e-04 | norm: 1.0116 dt: 535.84ms, tok/sec: 15288.08\n",
            "step  287, loss: 4.796885 | lr: 2.4000e-04 | norm: 1.0067 dt: 534.55ms, tok/sec: 15325.07\n",
            "step  288, loss: 4.854008 | lr: 2.4000e-04 | norm: 1.0733 dt: 536.64ms, tok/sec: 15265.44\n",
            "step  289, loss: 4.858647 | lr: 2.4000e-04 | norm: 1.1150 dt: 536.18ms, tok/sec: 15278.48\n",
            "step  290, loss: 4.504550 | lr: 2.4000e-04 | norm: 1.0390 dt: 535.47ms, tok/sec: 15298.82\n",
            "step  291, loss: 4.486996 | lr: 2.4000e-04 | norm: 0.8173 dt: 536.35ms, tok/sec: 15273.70\n",
            "step  292, loss: 4.609215 | lr: 2.4000e-04 | norm: 0.9835 dt: 537.61ms, tok/sec: 15237.95\n",
            "step  293, loss: 4.789359 | lr: 2.4000e-04 | norm: 1.1656 dt: 533.12ms, tok/sec: 15366.06\n",
            "step  294, loss: 4.610126 | lr: 2.4000e-04 | norm: 1.5430 dt: 532.88ms, tok/sec: 15373.19\n",
            "step  295, loss: 4.648255 | lr: 2.4000e-04 | norm: 1.0378 dt: 536.99ms, tok/sec: 15255.52\n",
            "step  296, loss: 4.397696 | lr: 2.4000e-04 | norm: 1.1845 dt: 535.77ms, tok/sec: 15290.05\n",
            "step  297, loss: 4.610015 | lr: 2.4000e-04 | norm: 1.2061 dt: 539.96ms, tok/sec: 15171.42\n",
            "step  298, loss: 4.514153 | lr: 2.4000e-04 | norm: 1.1360 dt: 534.58ms, tok/sec: 15324.10\n",
            "step  299, loss: 4.898467 | lr: 2.4000e-04 | norm: 0.8616 dt: 537.13ms, tok/sec: 15251.37\n",
            "Validation loss: 5.3952\n",
            "step  300, loss: 5.001698 | lr: 2.4000e-04 | norm: 1.4029 dt: 4084.51ms, tok/sec: 2005.63\n",
            "step  301, loss: 4.848141 | lr: 2.4000e-04 | norm: 1.1780 dt: 535.71ms, tok/sec: 15291.91\n",
            "step  302, loss: 4.818803 | lr: 2.4000e-04 | norm: 0.7980 dt: 537.95ms, tok/sec: 15228.29\n",
            "step  303, loss: 4.544640 | lr: 2.4000e-04 | norm: 1.1348 dt: 534.64ms, tok/sec: 15322.48\n",
            "step  304, loss: 4.734011 | lr: 2.4000e-04 | norm: 1.2171 dt: 537.87ms, tok/sec: 15230.56\n",
            "step  305, loss: 4.816524 | lr: 2.4000e-04 | norm: 0.9503 dt: 533.82ms, tok/sec: 15346.04\n",
            "step  306, loss: 4.756138 | lr: 2.4000e-04 | norm: 0.8529 dt: 537.28ms, tok/sec: 15247.11\n",
            "step  307, loss: 4.616983 | lr: 2.4000e-04 | norm: 1.2119 dt: 539.19ms, tok/sec: 15193.23\n",
            "step  308, loss: 4.787993 | lr: 2.4000e-04 | norm: 1.0849 dt: 536.99ms, tok/sec: 15255.49\n",
            "step  309, loss: 4.598723 | lr: 2.4000e-04 | norm: 1.2431 dt: 537.79ms, tok/sec: 15232.80\n",
            "step  310, loss: 4.840642 | lr: 2.4000e-04 | norm: 1.1384 dt: 539.32ms, tok/sec: 15189.56\n",
            "step  311, loss: 4.519782 | lr: 2.4000e-04 | norm: 0.9696 dt: 536.40ms, tok/sec: 15272.16\n",
            "step  312, loss: 4.533934 | lr: 2.4000e-04 | norm: 1.0000 dt: 539.63ms, tok/sec: 15180.69\n",
            "step  313, loss: 4.576961 | lr: 2.4000e-04 | norm: 0.8849 dt: 536.18ms, tok/sec: 15278.57\n",
            "step  314, loss: 4.938701 | lr: 2.4000e-04 | norm: 1.2998 dt: 537.22ms, tok/sec: 15248.75\n",
            "step  315, loss: 4.854339 | lr: 2.4000e-04 | norm: 1.0778 dt: 538.10ms, tok/sec: 15223.91\n",
            "step  316, loss: 5.185030 | lr: 2.4000e-04 | norm: 1.1589 dt: 535.50ms, tok/sec: 15297.92\n",
            "step  317, loss: 4.853072 | lr: 2.4000e-04 | norm: 1.1286 dt: 536.02ms, tok/sec: 15282.90\n",
            "step  318, loss: 4.898875 | lr: 2.4000e-04 | norm: 1.0054 dt: 535.75ms, tok/sec: 15290.61\n",
            "step  319, loss: 4.618373 | lr: 2.4000e-04 | norm: 1.1888 dt: 534.64ms, tok/sec: 15322.58\n",
            "step  320, loss: 4.533148 | lr: 2.4000e-04 | norm: 1.3287 dt: 535.51ms, tok/sec: 15297.45\n",
            "step  321, loss: 4.619943 | lr: 2.4000e-04 | norm: 1.0923 dt: 536.22ms, tok/sec: 15277.33\n",
            "step  322, loss: 4.451074 | lr: 2.4000e-04 | norm: 0.9753 dt: 537.90ms, tok/sec: 15229.48\n",
            "step  323, loss: 4.652751 | lr: 2.4000e-04 | norm: 0.9327 dt: 536.49ms, tok/sec: 15269.72\n",
            "step  324, loss: 4.719430 | lr: 2.4000e-04 | norm: 1.0084 dt: 538.14ms, tok/sec: 15222.94\n",
            "step  325, loss: 4.727904 | lr: 2.4000e-04 | norm: 0.9490 dt: 535.12ms, tok/sec: 15308.68\n",
            "step  326, loss: 4.373747 | lr: 2.4000e-04 | norm: 1.0338 dt: 538.54ms, tok/sec: 15211.41\n",
            "step  327, loss: 4.352828 | lr: 2.4000e-04 | norm: 0.8975 dt: 536.80ms, tok/sec: 15260.83\n",
            "step  328, loss: 4.464656 | lr: 2.4000e-04 | norm: 0.7854 dt: 535.97ms, tok/sec: 15284.46\n",
            "step  329, loss: 4.655586 | lr: 2.4000e-04 | norm: 0.8639 dt: 535.79ms, tok/sec: 15289.69\n",
            "step  330, loss: 4.479023 | lr: 2.4000e-04 | norm: 1.3817 dt: 534.38ms, tok/sec: 15330.04\n",
            "step  331, loss: 4.523184 | lr: 2.4000e-04 | norm: 1.2001 dt: 538.51ms, tok/sec: 15212.21\n",
            "step  332, loss: 4.266715 | lr: 2.4000e-04 | norm: 1.0399 dt: 537.99ms, tok/sec: 15227.04\n",
            "step  333, loss: 4.494116 | lr: 2.4000e-04 | norm: 0.9887 dt: 534.93ms, tok/sec: 15314.28\n",
            "step  334, loss: 4.390040 | lr: 2.4000e-04 | norm: 1.1724 dt: 538.64ms, tok/sec: 15208.68\n",
            "step  335, loss: 4.771236 | lr: 2.4000e-04 | norm: 0.9069 dt: 537.18ms, tok/sec: 15249.99\n",
            "step  336, loss: 4.867237 | lr: 2.4000e-04 | norm: 0.8668 dt: 536.66ms, tok/sec: 15264.90\n",
            "step  337, loss: 4.711170 | lr: 2.4000e-04 | norm: 0.9308 dt: 533.64ms, tok/sec: 15351.18\n",
            "step  338, loss: 4.686924 | lr: 2.4000e-04 | norm: 0.8624 dt: 536.76ms, tok/sec: 15261.99\n",
            "step  339, loss: 4.414961 | lr: 2.4000e-04 | norm: 0.8517 dt: 532.62ms, tok/sec: 15380.57\n",
            "step  340, loss: 4.621508 | lr: 2.4000e-04 | norm: 0.9852 dt: 538.26ms, tok/sec: 15219.31\n",
            "step  341, loss: 4.690951 | lr: 2.4000e-04 | norm: 0.9253 dt: 534.27ms, tok/sec: 15333.10\n",
            "step  342, loss: 4.630881 | lr: 2.4000e-04 | norm: 0.8700 dt: 538.47ms, tok/sec: 15213.39\n",
            "step  343, loss: 4.470772 | lr: 2.4000e-04 | norm: 0.8712 dt: 534.47ms, tok/sec: 15327.41\n",
            "step  344, loss: 4.644333 | lr: 2.4000e-04 | norm: 0.9104 dt: 541.77ms, tok/sec: 15120.87\n",
            "step  345, loss: 4.452524 | lr: 2.4000e-04 | norm: 1.0979 dt: 536.69ms, tok/sec: 15263.84\n",
            "step  346, loss: 4.719314 | lr: 2.4000e-04 | norm: 1.0295 dt: 541.42ms, tok/sec: 15130.54\n",
            "step  347, loss: 4.381779 | lr: 2.4000e-04 | norm: 0.9574 dt: 535.78ms, tok/sec: 15289.90\n",
            "step  348, loss: 4.395681 | lr: 2.4000e-04 | norm: 0.9890 dt: 541.44ms, tok/sec: 15130.03\n",
            "step  349, loss: 4.433778 | lr: 2.4000e-04 | norm: 0.8832 dt: 538.48ms, tok/sec: 15213.29\n",
            "step  350, loss: 4.818670 | lr: 2.4000e-04 | norm: 1.2843 dt: 538.03ms, tok/sec: 15225.94\n",
            "step  351, loss: 4.735450 | lr: 2.4000e-04 | norm: 1.0872 dt: 535.24ms, tok/sec: 15305.37\n",
            "step  352, loss: 5.063112 | lr: 2.4000e-04 | norm: 1.1413 dt: 537.44ms, tok/sec: 15242.60\n",
            "step  353, loss: 4.712259 | lr: 2.4000e-04 | norm: 1.0190 dt: 535.88ms, tok/sec: 15287.08\n",
            "step  354, loss: 4.759171 | lr: 2.4000e-04 | norm: 0.8536 dt: 541.26ms, tok/sec: 15135.02\n",
            "step  355, loss: 4.492602 | lr: 2.4000e-04 | norm: 1.1308 dt: 534.47ms, tok/sec: 15327.30\n",
            "step  356, loss: 4.403135 | lr: 2.4000e-04 | norm: 1.2568 dt: 538.25ms, tok/sec: 15219.59\n",
            "step  357, loss: 4.481565 | lr: 2.4000e-04 | norm: 1.1348 dt: 536.11ms, tok/sec: 15280.42\n",
            "step  358, loss: 4.323882 | lr: 2.4000e-04 | norm: 0.9453 dt: 539.09ms, tok/sec: 15196.02\n",
            "step  359, loss: 4.532335 | lr: 2.4000e-04 | norm: 0.8631 dt: 536.03ms, tok/sec: 15282.64\n",
            "step  360, loss: 4.582658 | lr: 2.4000e-04 | norm: 0.9456 dt: 543.67ms, tok/sec: 15068.08\n",
            "step  361, loss: 4.585766 | lr: 2.4000e-04 | norm: 0.9270 dt: 533.00ms, tok/sec: 15369.50\n",
            "step  362, loss: 4.242596 | lr: 2.4000e-04 | norm: 0.9057 dt: 541.16ms, tok/sec: 15137.81\n",
            "step  363, loss: 4.223940 | lr: 2.4000e-04 | norm: 0.8886 dt: 537.13ms, tok/sec: 15251.53\n",
            "step  364, loss: 4.340978 | lr: 2.4000e-04 | norm: 0.8763 dt: 539.13ms, tok/sec: 15194.74\n",
            "step  365, loss: 4.520964 | lr: 2.4000e-04 | norm: 0.8361 dt: 536.00ms, tok/sec: 15283.60\n",
            "step  366, loss: 4.345703 | lr: 2.4000e-04 | norm: 1.1514 dt: 538.35ms, tok/sec: 15216.74\n",
            "step  367, loss: 4.384575 | lr: 2.4000e-04 | norm: 1.0474 dt: 534.84ms, tok/sec: 15316.66\n",
            "step  368, loss: 4.118969 | lr: 2.4000e-04 | norm: 1.0214 dt: 541.14ms, tok/sec: 15138.55\n",
            "step  369, loss: 4.345737 | lr: 2.4000e-04 | norm: 0.8851 dt: 537.86ms, tok/sec: 15230.62\n",
            "step  370, loss: 4.233760 | lr: 2.4000e-04 | norm: 1.0146 dt: 540.38ms, tok/sec: 15159.58\n",
            "step  371, loss: 4.638856 | lr: 2.4000e-04 | norm: 0.9323 dt: 535.88ms, tok/sec: 15287.13\n",
            "step  372, loss: 4.714455 | lr: 2.4000e-04 | norm: 0.9225 dt: 536.94ms, tok/sec: 15256.81\n",
            "step  373, loss: 4.568778 | lr: 2.4000e-04 | norm: 0.8665 dt: 540.42ms, tok/sec: 15158.63\n",
            "step  374, loss: 4.559362 | lr: 2.4000e-04 | norm: 0.8508 dt: 540.77ms, tok/sec: 15148.75\n",
            "step  375, loss: 4.279800 | lr: 2.4000e-04 | norm: 0.9645 dt: 537.51ms, tok/sec: 15240.64\n",
            "step  376, loss: 4.479432 | lr: 2.4000e-04 | norm: 0.9389 dt: 538.33ms, tok/sec: 15217.33\n",
            "step  377, loss: 4.559432 | lr: 2.4000e-04 | norm: 0.8819 dt: 537.43ms, tok/sec: 15243.06\n",
            "step  378, loss: 4.497101 | lr: 2.4000e-04 | norm: 1.0663 dt: 540.16ms, tok/sec: 15165.87\n",
            "step  379, loss: 4.334070 | lr: 2.4000e-04 | norm: 0.8877 dt: 532.72ms, tok/sec: 15377.66\n",
            "step  380, loss: 4.507689 | lr: 2.4000e-04 | norm: 0.8956 dt: 539.53ms, tok/sec: 15183.65\n",
            "step  381, loss: 4.307140 | lr: 2.4000e-04 | norm: 0.9506 dt: 537.99ms, tok/sec: 15227.03\n",
            "step  382, loss: 4.588565 | lr: 2.4000e-04 | norm: 1.0601 dt: 544.15ms, tok/sec: 15054.68\n",
            "step  383, loss: 4.254938 | lr: 2.4000e-04 | norm: 0.9730 dt: 538.00ms, tok/sec: 15226.82\n",
            "step  384, loss: 4.276025 | lr: 2.4000e-04 | norm: 1.0312 dt: 538.35ms, tok/sec: 15217.00\n",
            "step  385, loss: 4.308348 | lr: 2.4000e-04 | norm: 0.9683 dt: 537.98ms, tok/sec: 15227.22\n",
            "step  386, loss: 4.673043 | lr: 2.4000e-04 | norm: 1.2715 dt: 539.86ms, tok/sec: 15174.28\n",
            "step  387, loss: 4.607786 | lr: 2.4000e-04 | norm: 1.0528 dt: 540.30ms, tok/sec: 15161.82\n",
            "step  388, loss: 4.945229 | lr: 2.4000e-04 | norm: 1.2675 dt: 539.04ms, tok/sec: 15197.29\n",
            "step  389, loss: 4.604070 | lr: 2.4000e-04 | norm: 1.3422 dt: 536.60ms, tok/sec: 15266.37\n",
            "step  390, loss: 4.656135 | lr: 2.4000e-04 | norm: 1.1544 dt: 539.96ms, tok/sec: 15171.44\n",
            "step  391, loss: 4.352921 | lr: 2.4000e-04 | norm: 1.1506 dt: 539.16ms, tok/sec: 15193.94\n",
            "step  392, loss: 4.267253 | lr: 2.4000e-04 | norm: 1.1217 dt: 541.54ms, tok/sec: 15127.27\n",
            "step  393, loss: 4.363290 | lr: 2.4000e-04 | norm: 1.1729 dt: 536.06ms, tok/sec: 15281.99\n",
            "step  394, loss: 4.207454 | lr: 2.4000e-04 | norm: 1.1418 dt: 539.70ms, tok/sec: 15178.67\n",
            "step  395, loss: 4.413545 | lr: 2.4000e-04 | norm: 0.9981 dt: 537.44ms, tok/sec: 15242.61\n",
            "step  396, loss: 4.466743 | lr: 2.4000e-04 | norm: 1.0110 dt: 539.26ms, tok/sec: 15191.14\n",
            "step  397, loss: 4.465979 | lr: 2.4000e-04 | norm: 0.9623 dt: 538.16ms, tok/sec: 15222.26\n",
            "step  398, loss: 4.140163 | lr: 2.4000e-04 | norm: 1.1109 dt: 541.97ms, tok/sec: 15115.19\n",
            "step  399, loss: 4.110209 | lr: 2.4000e-04 | norm: 1.0761 dt: 539.26ms, tok/sec: 15191.12\n",
            "Validation loss: 5.3483\n",
            "step  400, loss: 4.236898 | lr: 2.4000e-04 | norm: 0.9424 dt: 4119.21ms, tok/sec: 1988.73\n",
            "step  401, loss: 4.412893 | lr: 2.4000e-04 | norm: 0.9997 dt: 534.43ms, tok/sec: 15328.47\n",
            "step  402, loss: 4.220508 | lr: 2.4000e-04 | norm: 1.2529 dt: 540.81ms, tok/sec: 15147.74\n",
            "step  403, loss: 4.275501 | lr: 2.4000e-04 | norm: 1.2014 dt: 533.61ms, tok/sec: 15352.03\n",
            "step  404, loss: 4.008324 | lr: 2.4000e-04 | norm: 1.0865 dt: 539.08ms, tok/sec: 15196.27\n",
            "step  405, loss: 4.236365 | lr: 2.4000e-04 | norm: 1.0489 dt: 532.93ms, tok/sec: 15371.53\n",
            "step  406, loss: 4.122113 | lr: 2.4000e-04 | norm: 1.1094 dt: 537.39ms, tok/sec: 15243.91\n",
            "step  407, loss: 4.523808 | lr: 2.4000e-04 | norm: 1.0100 dt: 538.09ms, tok/sec: 15224.08\n",
            "step  408, loss: 4.597834 | lr: 2.4000e-04 | norm: 1.0885 dt: 536.51ms, tok/sec: 15269.04\n",
            "step  409, loss: 4.448301 | lr: 2.4000e-04 | norm: 0.9876 dt: 538.09ms, tok/sec: 15224.28\n",
            "step  410, loss: 4.446258 | lr: 2.4000e-04 | norm: 1.0425 dt: 536.18ms, tok/sec: 15278.51\n",
            "step  411, loss: 4.170179 | lr: 2.4000e-04 | norm: 0.9578 dt: 534.79ms, tok/sec: 15318.29\n",
            "step  412, loss: 4.369190 | lr: 2.4000e-04 | norm: 0.9804 dt: 535.65ms, tok/sec: 15293.69\n",
            "step  413, loss: 4.440344 | lr: 2.4000e-04 | norm: 1.0194 dt: 532.94ms, tok/sec: 15371.33\n",
            "step  414, loss: 4.381251 | lr: 2.4000e-04 | norm: 1.0115 dt: 537.96ms, tok/sec: 15227.96\n",
            "step  415, loss: 4.215279 | lr: 2.4000e-04 | norm: 1.1458 dt: 538.61ms, tok/sec: 15209.47\n",
            "step  416, loss: 4.389576 | lr: 2.4000e-04 | norm: 1.0695 dt: 541.70ms, tok/sec: 15122.84\n",
            "step  417, loss: 4.182552 | lr: 2.4000e-04 | norm: 0.9119 dt: 533.06ms, tok/sec: 15367.81\n",
            "step  418, loss: 4.483356 | lr: 2.4000e-04 | norm: 1.2254 dt: 539.50ms, tok/sec: 15184.32\n",
            "step  419, loss: 4.169184 | lr: 2.4000e-04 | norm: 1.3557 dt: 536.33ms, tok/sec: 15274.26\n",
            "step  420, loss: 4.190880 | lr: 2.4000e-04 | norm: 1.0705 dt: 539.79ms, tok/sec: 15176.17\n",
            "step  421, loss: 4.222610 | lr: 2.4000e-04 | norm: 1.3486 dt: 531.50ms, tok/sec: 15412.95\n",
            "step  422, loss: 4.619684 | lr: 2.4000e-04 | norm: 2.0159 dt: 538.31ms, tok/sec: 15218.09\n",
            "step  423, loss: 4.544575 | lr: 2.4000e-04 | norm: 1.8413 dt: 536.49ms, tok/sec: 15269.52\n",
            "step  424, loss: 4.861288 | lr: 2.4000e-04 | norm: 1.4576 dt: 533.46ms, tok/sec: 15356.43\n",
            "step  425, loss: 4.525680 | lr: 2.4000e-04 | norm: 1.3309 dt: 539.74ms, tok/sec: 15177.72\n",
            "step  426, loss: 4.608965 | lr: 2.4000e-04 | norm: 1.6532 dt: 534.93ms, tok/sec: 15314.18\n",
            "step  427, loss: 4.300551 | lr: 2.4000e-04 | norm: 1.7911 dt: 532.76ms, tok/sec: 15376.62\n",
            "step  428, loss: 4.201476 | lr: 2.4000e-04 | norm: 1.6095 dt: 533.27ms, tok/sec: 15361.83\n",
            "step  429, loss: 4.294785 | lr: 2.4000e-04 | norm: 1.4099 dt: 538.70ms, tok/sec: 15207.02\n",
            "step  430, loss: 4.117796 | lr: 2.4000e-04 | norm: 1.1837 dt: 534.11ms, tok/sec: 15337.69\n",
            "step  431, loss: 4.325243 | lr: 2.4000e-04 | norm: 1.2043 dt: 539.18ms, tok/sec: 15193.55\n",
            "step  432, loss: 4.371649 | lr: 2.4000e-04 | norm: 1.3335 dt: 535.26ms, tok/sec: 15304.63\n",
            "step  433, loss: 4.378419 | lr: 2.4000e-04 | norm: 1.1648 dt: 534.27ms, tok/sec: 15332.93\n",
            "step  434, loss: 4.059847 | lr: 2.4000e-04 | norm: 1.1829 dt: 531.51ms, tok/sec: 15412.83\n",
            "step  435, loss: 4.033751 | lr: 2.4000e-04 | norm: 1.2859 dt: 538.62ms, tok/sec: 15209.12\n",
            "step  436, loss: 4.137491 | lr: 2.4000e-04 | norm: 1.1416 dt: 532.18ms, tok/sec: 15393.35\n",
            "step  437, loss: 4.310155 | lr: 2.4000e-04 | norm: 1.0079 dt: 538.53ms, tok/sec: 15211.86\n",
            "step  438, loss: 4.108577 | lr: 2.4000e-04 | norm: 1.1012 dt: 533.45ms, tok/sec: 15356.60\n",
            "step  439, loss: 4.162160 | lr: 2.4000e-04 | norm: 1.0653 dt: 535.73ms, tok/sec: 15291.31\n",
            "step  440, loss: 3.918399 | lr: 2.4000e-04 | norm: 1.0654 dt: 529.87ms, tok/sec: 15460.35\n",
            "step  441, loss: 4.141058 | lr: 2.4000e-04 | norm: 1.1515 dt: 535.77ms, tok/sec: 15290.08\n",
            "step  442, loss: 4.026573 | lr: 2.4000e-04 | norm: 1.0532 dt: 535.26ms, tok/sec: 15304.68\n",
            "step  443, loss: 4.409118 | lr: 2.4000e-04 | norm: 1.0212 dt: 535.59ms, tok/sec: 15295.27\n",
            "step  444, loss: 4.485739 | lr: 2.4000e-04 | norm: 1.1217 dt: 535.41ms, tok/sec: 15300.45\n",
            "step  445, loss: 4.341750 | lr: 2.4000e-04 | norm: 1.2720 dt: 534.31ms, tok/sec: 15331.87\n",
            "step  446, loss: 4.340986 | lr: 2.4000e-04 | norm: 1.0439 dt: 533.74ms, tok/sec: 15348.23\n",
            "step  447, loss: 4.061621 | lr: 2.4000e-04 | norm: 1.0027 dt: 532.96ms, tok/sec: 15370.67\n",
            "step  448, loss: 4.275705 | lr: 2.4000e-04 | norm: 1.1363 dt: 532.35ms, tok/sec: 15388.45\n",
            "step  449, loss: 4.335081 | lr: 2.4000e-04 | norm: 0.9665 dt: 534.24ms, tok/sec: 15334.01\n",
            "step  450, loss: 4.258900 | lr: 2.4000e-04 | norm: 1.0658 dt: 531.78ms, tok/sec: 15404.95\n",
            "step  451, loss: 4.116636 | lr: 2.4000e-04 | norm: 1.0642 dt: 538.11ms, tok/sec: 15223.66\n",
            "step  452, loss: 4.295331 | lr: 2.4000e-04 | norm: 1.2276 dt: 533.45ms, tok/sec: 15356.62\n",
            "step  453, loss: 4.100904 | lr: 2.4000e-04 | norm: 1.4857 dt: 537.05ms, tok/sec: 15253.78\n",
            "step  454, loss: 4.392914 | lr: 2.4000e-04 | norm: 1.1502 dt: 529.91ms, tok/sec: 15459.18\n",
            "step  455, loss: 4.086807 | lr: 2.4000e-04 | norm: 1.0890 dt: 537.89ms, tok/sec: 15229.94\n",
            "step  456, loss: 4.104180 | lr: 2.4000e-04 | norm: 1.1436 dt: 537.59ms, tok/sec: 15238.34\n",
            "step  457, loss: 4.143237 | lr: 2.4000e-04 | norm: 1.3694 dt: 534.29ms, tok/sec: 15332.39\n",
            "step  458, loss: 4.537653 | lr: 2.4000e-04 | norm: 1.6586 dt: 532.77ms, tok/sec: 15376.28\n",
            "step  459, loss: 4.479560 | lr: 2.4000e-04 | norm: 1.6961 dt: 533.31ms, tok/sec: 15360.55\n",
            "step  460, loss: 4.768399 | lr: 2.4000e-04 | norm: 1.5515 dt: 537.93ms, tok/sec: 15228.77\n",
            "step  461, loss: 4.427235 | lr: 2.4000e-04 | norm: 1.3656 dt: 534.47ms, tok/sec: 15327.40\n",
            "step  462, loss: 4.525669 | lr: 2.4000e-04 | norm: 1.4356 dt: 534.51ms, tok/sec: 15326.24\n",
            "step  463, loss: 4.204775 | lr: 2.4000e-04 | norm: 1.5215 dt: 533.09ms, tok/sec: 15366.96\n",
            "step  464, loss: 4.105952 | lr: 2.4000e-04 | norm: 1.2303 dt: 536.97ms, tok/sec: 15256.10\n",
            "step  465, loss: 4.198679 | lr: 2.4000e-04 | norm: 1.2002 dt: 531.53ms, tok/sec: 15412.03\n",
            "step  466, loss: 4.034158 | lr: 2.4000e-04 | norm: 1.1097 dt: 534.71ms, tok/sec: 15320.42\n",
            "step  467, loss: 4.251232 | lr: 2.4000e-04 | norm: 1.2058 dt: 537.43ms, tok/sec: 15242.78\n",
            "step  468, loss: 4.307462 | lr: 2.4000e-04 | norm: 1.4783 dt: 532.47ms, tok/sec: 15384.82\n",
            "step  469, loss: 4.281435 | lr: 2.4000e-04 | norm: 1.2717 dt: 538.90ms, tok/sec: 15201.20\n",
            "step  470, loss: 3.965006 | lr: 2.4000e-04 | norm: 1.0254 dt: 533.15ms, tok/sec: 15365.27\n",
            "step  471, loss: 3.931872 | lr: 2.4000e-04 | norm: 1.0025 dt: 538.43ms, tok/sec: 15214.67\n",
            "step  472, loss: 4.049060 | lr: 2.4000e-04 | norm: 1.1580 dt: 530.77ms, tok/sec: 15434.23\n",
            "step  473, loss: 4.226148 | lr: 2.4000e-04 | norm: 1.2170 dt: 540.52ms, tok/sec: 15155.68\n",
            "step  474, loss: 4.050568 | lr: 2.4000e-04 | norm: 1.3495 dt: 529.83ms, tok/sec: 15461.65\n",
            "step  475, loss: 4.080393 | lr: 2.4000e-04 | norm: 1.2736 dt: 537.86ms, tok/sec: 15230.63\n",
            "step  476, loss: 3.830690 | lr: 2.4000e-04 | norm: 1.0084 dt: 534.48ms, tok/sec: 15327.11\n",
            "step  477, loss: 4.060453 | lr: 2.4000e-04 | norm: 1.0978 dt: 538.05ms, tok/sec: 15225.33\n",
            "step  478, loss: 3.913041 | lr: 2.4000e-04 | norm: 1.1041 dt: 534.52ms, tok/sec: 15325.97\n",
            "step  479, loss: 4.292403 | lr: 2.4000e-04 | norm: 1.2080 dt: 539.22ms, tok/sec: 15192.21\n",
            "step  480, loss: 4.370248 | lr: 2.4000e-04 | norm: 1.1328 dt: 534.23ms, tok/sec: 15334.23\n",
            "step  481, loss: 4.249280 | lr: 2.4000e-04 | norm: 1.2349 dt: 537.50ms, tok/sec: 15240.87\n",
            "step  482, loss: 4.230927 | lr: 2.4000e-04 | norm: 1.4018 dt: 532.91ms, tok/sec: 15372.25\n",
            "step  483, loss: 3.962465 | lr: 2.4000e-04 | norm: 1.1676 dt: 537.31ms, tok/sec: 15246.32\n",
            "step  484, loss: 4.179251 | lr: 2.4000e-04 | norm: 1.1546 dt: 531.93ms, tok/sec: 15400.65\n",
            "step  485, loss: 4.230978 | lr: 2.4000e-04 | norm: 1.0626 dt: 534.18ms, tok/sec: 15335.61\n",
            "step  486, loss: 4.156085 | lr: 2.4000e-04 | norm: 1.1855 dt: 534.91ms, tok/sec: 15314.86\n",
            "step  487, loss: 4.027915 | lr: 2.4000e-04 | norm: 1.2998 dt: 532.24ms, tok/sec: 15391.43\n",
            "step  488, loss: 4.215372 | lr: 2.4000e-04 | norm: 1.4944 dt: 533.14ms, tok/sec: 15365.56\n",
            "step  489, loss: 4.036489 | lr: 2.4000e-04 | norm: 1.6423 dt: 532.63ms, tok/sec: 15380.15\n",
            "step  490, loss: 4.306908 | lr: 2.4000e-04 | norm: 1.5188 dt: 535.33ms, tok/sec: 15302.78\n",
            "step  491, loss: 3.982290 | lr: 2.4000e-04 | norm: 1.0793 dt: 534.90ms, tok/sec: 15314.97\n",
            "step  492, loss: 4.014539 | lr: 2.4000e-04 | norm: 1.1491 dt: 533.19ms, tok/sec: 15364.20\n",
            "step  493, loss: 4.080617 | lr: 2.4000e-04 | norm: 1.4796 dt: 535.09ms, tok/sec: 15309.70\n",
            "step  494, loss: 4.447792 | lr: 2.4000e-04 | norm: 1.8287 dt: 539.70ms, tok/sec: 15178.90\n",
            "step  495, loss: 4.409626 | lr: 2.4000e-04 | norm: 1.6352 dt: 537.45ms, tok/sec: 15242.38\n",
            "step  496, loss: 4.674458 | lr: 2.4000e-04 | norm: 1.8555 dt: 535.35ms, tok/sec: 15302.25\n",
            "step  497, loss: 4.314346 | lr: 2.4000e-04 | norm: 1.3616 dt: 534.31ms, tok/sec: 15331.79\n",
            "step  498, loss: 4.420376 | lr: 2.4000e-04 | norm: 1.1697 dt: 535.40ms, tok/sec: 15300.65\n",
            "step  499, loss: 4.098094 | lr: 2.4000e-04 | norm: 1.5055 dt: 529.17ms, tok/sec: 15480.89\n",
            "Validation loss: 5.3976\n",
            "step  500, loss: 4.003122 | lr: 2.4000e-04 | norm: 1.3144 dt: 4097.73ms, tok/sec: 1999.15\n",
            "step  501, loss: 4.085640 | lr: 2.4000e-04 | norm: 1.3910 dt: 530.43ms, tok/sec: 15444.18\n",
            "step  502, loss: 3.938452 | lr: 2.4000e-04 | norm: 1.1609 dt: 536.40ms, tok/sec: 15272.28\n",
            "step  503, loss: 4.151453 | lr: 2.4000e-04 | norm: 1.1584 dt: 531.96ms, tok/sec: 15399.69\n",
            "step  504, loss: 4.212521 | lr: 2.4000e-04 | norm: 1.3370 dt: 540.35ms, tok/sec: 15160.52\n",
            "step  505, loss: 4.191379 | lr: 2.4000e-04 | norm: 1.3557 dt: 532.98ms, tok/sec: 15370.09\n",
            "step  506, loss: 3.883426 | lr: 2.4000e-04 | norm: 1.2130 dt: 538.46ms, tok/sec: 15213.70\n",
            "step  507, loss: 3.834403 | lr: 2.4000e-04 | norm: 1.0398 dt: 530.90ms, tok/sec: 15430.53\n",
            "step  508, loss: 3.945319 | lr: 2.4000e-04 | norm: 0.9858 dt: 538.32ms, tok/sec: 15217.64\n",
            "step  509, loss: 4.120245 | lr: 2.4000e-04 | norm: 1.1395 dt: 532.48ms, tok/sec: 15384.51\n",
            "step  510, loss: 3.948916 | lr: 2.4000e-04 | norm: 1.3398 dt: 537.56ms, tok/sec: 15239.11\n",
            "step  511, loss: 3.989816 | lr: 2.4000e-04 | norm: 1.2688 dt: 537.91ms, tok/sec: 15229.30\n",
            "step  512, loss: 3.730807 | lr: 2.4000e-04 | norm: 1.1932 dt: 538.30ms, tok/sec: 15218.25\n",
            "step  513, loss: 3.970899 | lr: 2.4000e-04 | norm: 1.1472 dt: 539.05ms, tok/sec: 15197.24\n",
            "step  514, loss: 3.839109 | lr: 2.4000e-04 | norm: 1.1392 dt: 542.31ms, tok/sec: 15105.87\n",
            "step  515, loss: 4.210046 | lr: 2.4000e-04 | norm: 1.2614 dt: 535.92ms, tok/sec: 15285.73\n",
            "step  516, loss: 4.285912 | lr: 2.4000e-04 | norm: 1.4121 dt: 540.87ms, tok/sec: 15145.86\n",
            "step  517, loss: 4.154593 | lr: 2.4000e-04 | norm: 1.3039 dt: 535.18ms, tok/sec: 15306.92\n",
            "step  518, loss: 4.136313 | lr: 2.4000e-04 | norm: 1.2240 dt: 540.67ms, tok/sec: 15151.57\n",
            "step  519, loss: 3.867974 | lr: 2.4000e-04 | norm: 1.2654 dt: 533.11ms, tok/sec: 15366.33\n",
            "step  520, loss: 4.082156 | lr: 2.4000e-04 | norm: 1.5272 dt: 537.65ms, tok/sec: 15236.58\n",
            "step  521, loss: 4.156243 | lr: 2.4000e-04 | norm: 1.5653 dt: 534.25ms, tok/sec: 15333.75\n",
            "step  522, loss: 4.075791 | lr: 2.4000e-04 | norm: 1.3845 dt: 540.20ms, tok/sec: 15164.83\n",
            "step  523, loss: 3.956513 | lr: 2.4000e-04 | norm: 1.3490 dt: 537.40ms, tok/sec: 15243.85\n",
            "step  524, loss: 4.144439 | lr: 2.4000e-04 | norm: 1.3182 dt: 543.04ms, tok/sec: 15085.43\n",
            "step  525, loss: 3.967113 | lr: 2.4000e-04 | norm: 1.4437 dt: 530.49ms, tok/sec: 15442.33\n",
            "step  526, loss: 4.253552 | lr: 2.4000e-04 | norm: 1.6343 dt: 539.47ms, tok/sec: 15185.30\n",
            "step  527, loss: 3.926064 | lr: 2.4000e-04 | norm: 1.4810 dt: 533.94ms, tok/sec: 15342.46\n",
            "step  528, loss: 3.953337 | lr: 2.4000e-04 | norm: 1.3700 dt: 538.81ms, tok/sec: 15203.77\n",
            "step  529, loss: 3.997436 | lr: 2.4000e-04 | norm: 1.2226 dt: 539.93ms, tok/sec: 15172.38\n",
            "step  530, loss: 4.329287 | lr: 2.4000e-04 | norm: 1.3869 dt: 537.06ms, tok/sec: 15253.53\n",
            "step  531, loss: 4.292253 | lr: 2.4000e-04 | norm: 1.4162 dt: 536.93ms, tok/sec: 15257.25\n",
            "step  532, loss: 4.561377 | lr: 2.4000e-04 | norm: 1.6686 dt: 534.67ms, tok/sec: 15321.60\n",
            "step  533, loss: 4.215720 | lr: 2.4000e-04 | norm: 1.5696 dt: 537.16ms, tok/sec: 15250.52\n",
            "step  534, loss: 4.333115 | lr: 2.4000e-04 | norm: 1.5911 dt: 533.04ms, tok/sec: 15368.42\n",
            "step  535, loss: 4.010796 | lr: 2.4000e-04 | norm: 1.4883 dt: 539.77ms, tok/sec: 15176.91\n",
            "step  536, loss: 3.923090 | lr: 2.4000e-04 | norm: 1.3357 dt: 534.13ms, tok/sec: 15337.01\n",
            "step  537, loss: 3.983568 | lr: 2.4000e-04 | norm: 1.2589 dt: 538.52ms, tok/sec: 15212.08\n",
            "step  538, loss: 3.829212 | lr: 2.4000e-04 | norm: 1.1816 dt: 534.49ms, tok/sec: 15326.67\n",
            "step  539, loss: 4.043819 | lr: 2.4000e-04 | norm: 1.2266 dt: 539.35ms, tok/sec: 15188.59\n",
            "step  540, loss: 4.125895 | lr: 2.4000e-04 | norm: 1.4212 dt: 536.21ms, tok/sec: 15277.49\n",
            "step  541, loss: 4.119681 | lr: 2.4000e-04 | norm: 1.4271 dt: 536.09ms, tok/sec: 15281.07\n",
            "step  542, loss: 3.819528 | lr: 2.4000e-04 | norm: 1.4711 dt: 540.84ms, tok/sec: 15146.82\n",
            "step  543, loss: 3.755136 | lr: 2.4000e-04 | norm: 1.4059 dt: 536.45ms, tok/sec: 15270.67\n",
            "step  544, loss: 3.849511 | lr: 2.4000e-04 | norm: 1.2948 dt: 535.82ms, tok/sec: 15288.59\n",
            "step  545, loss: 4.042139 | lr: 2.4000e-04 | norm: 1.3503 dt: 537.50ms, tok/sec: 15240.91\n",
            "step  546, loss: 3.887987 | lr: 2.4000e-04 | norm: 1.5797 dt: 536.84ms, tok/sec: 15259.65\n",
            "step  547, loss: 3.925181 | lr: 2.4000e-04 | norm: 1.4291 dt: 537.77ms, tok/sec: 15233.23\n",
            "step  548, loss: 3.666935 | lr: 2.4000e-04 | norm: 1.2621 dt: 535.52ms, tok/sec: 15297.20\n",
            "step  549, loss: 3.895248 | lr: 2.4000e-04 | norm: 1.3321 dt: 539.32ms, tok/sec: 15189.63\n",
            "step  550, loss: 3.762069 | lr: 2.4000e-04 | norm: 1.4314 dt: 536.29ms, tok/sec: 15275.26\n",
            "step  551, loss: 4.118153 | lr: 2.4000e-04 | norm: 1.3580 dt: 537.63ms, tok/sec: 15237.29\n",
            "step  552, loss: 4.201812 | lr: 2.4000e-04 | norm: 1.3435 dt: 536.80ms, tok/sec: 15260.75\n",
            "step  553, loss: 4.068366 | lr: 2.4000e-04 | norm: 1.2735 dt: 539.96ms, tok/sec: 15171.43\n",
            "step  554, loss: 4.049377 | lr: 2.4000e-04 | norm: 1.3278 dt: 535.97ms, tok/sec: 15284.32\n",
            "step  555, loss: 3.802393 | lr: 2.4000e-04 | norm: 1.2987 dt: 539.16ms, tok/sec: 15194.11\n",
            "step  556, loss: 4.019540 | lr: 2.4000e-04 | norm: 1.4322 dt: 535.25ms, tok/sec: 15305.08\n",
            "step  557, loss: 4.083723 | lr: 2.4000e-04 | norm: 1.3319 dt: 536.24ms, tok/sec: 15276.75\n",
            "step  558, loss: 3.985898 | lr: 2.4000e-04 | norm: 1.2936 dt: 537.03ms, tok/sec: 15254.35\n",
            "step  559, loss: 3.848008 | lr: 2.4000e-04 | norm: 1.1379 dt: 536.18ms, tok/sec: 15278.44\n",
            "step  560, loss: 4.027689 | lr: 2.4000e-04 | norm: 1.2022 dt: 538.32ms, tok/sec: 15217.82\n",
            "step  561, loss: 3.870758 | lr: 2.4000e-04 | norm: 1.3348 dt: 536.64ms, tok/sec: 15265.36\n",
            "step  562, loss: 4.158366 | lr: 2.4000e-04 | norm: 1.4581 dt: 539.79ms, tok/sec: 15176.35\n",
            "step  563, loss: 3.836359 | lr: 2.4000e-04 | norm: 1.4085 dt: 538.01ms, tok/sec: 15226.47\n",
            "step  564, loss: 3.868907 | lr: 2.4000e-04 | norm: 1.3193 dt: 538.07ms, tok/sec: 15224.69\n",
            "step  565, loss: 3.901534 | lr: 2.4000e-04 | norm: 1.3285 dt: 538.83ms, tok/sec: 15203.26\n",
            "step  566, loss: 4.238398 | lr: 2.4000e-04 | norm: 1.5629 dt: 539.42ms, tok/sec: 15186.81\n",
            "step  567, loss: 4.208033 | lr: 2.4000e-04 | norm: 1.4693 dt: 538.04ms, tok/sec: 15225.70\n",
            "step  568, loss: 4.430890 | lr: 2.4000e-04 | norm: 1.5641 dt: 540.46ms, tok/sec: 15157.59\n",
            "step  569, loss: 4.084005 | lr: 2.4000e-04 | norm: 1.4765 dt: 536.00ms, tok/sec: 15283.50\n",
            "step  570, loss: 4.210585 | lr: 2.4000e-04 | norm: 1.4454 dt: 537.95ms, tok/sec: 15228.25\n",
            "step  571, loss: 3.885735 | lr: 2.4000e-04 | norm: 1.4414 dt: 539.76ms, tok/sec: 15177.21\n",
            "step  572, loss: 3.836977 | lr: 2.4000e-04 | norm: 1.2984 dt: 541.64ms, tok/sec: 15124.38\n",
            "step  573, loss: 3.884053 | lr: 2.4000e-04 | norm: 1.4750 dt: 533.87ms, tok/sec: 15344.68\n",
            "step  574, loss: 3.726988 | lr: 2.4000e-04 | norm: 1.1941 dt: 535.40ms, tok/sec: 15300.75\n",
            "step  575, loss: 3.936574 | lr: 2.4000e-04 | norm: 1.2268 dt: 539.02ms, tok/sec: 15197.96\n",
            "step  576, loss: 4.008099 | lr: 2.4000e-04 | norm: 1.4517 dt: 539.05ms, tok/sec: 15197.01\n",
            "step  577, loss: 3.998712 | lr: 2.4000e-04 | norm: 1.3498 dt: 538.29ms, tok/sec: 15218.60\n",
            "step  578, loss: 3.721341 | lr: 2.4000e-04 | norm: 1.3294 dt: 535.31ms, tok/sec: 15303.25\n",
            "step  579, loss: 3.685368 | lr: 2.4000e-04 | norm: 1.4028 dt: 539.63ms, tok/sec: 15180.75\n",
            "step  580, loss: 3.783787 | lr: 2.4000e-04 | norm: 1.5647 dt: 537.01ms, tok/sec: 15254.78\n",
            "step  581, loss: 3.981043 | lr: 2.4000e-04 | norm: 1.5570 dt: 538.84ms, tok/sec: 15203.15\n",
            "step  582, loss: 3.840181 | lr: 2.4000e-04 | norm: 1.5881 dt: 538.20ms, tok/sec: 15221.10\n",
            "step  583, loss: 3.861817 | lr: 2.4000e-04 | norm: 1.6731 dt: 538.86ms, tok/sec: 15202.57\n",
            "step  584, loss: 3.587646 | lr: 2.4000e-04 | norm: 1.4230 dt: 535.16ms, tok/sec: 15307.45\n",
            "step  585, loss: 3.821547 | lr: 2.4000e-04 | norm: 1.4256 dt: 538.85ms, tok/sec: 15202.83\n",
            "step  586, loss: 3.701778 | lr: 2.4000e-04 | norm: 1.4773 dt: 538.76ms, tok/sec: 15205.38\n",
            "step  587, loss: 4.038484 | lr: 2.4000e-04 | norm: 1.5785 dt: 541.09ms, tok/sec: 15139.94\n",
            "step  588, loss: 4.114142 | lr: 2.4000e-04 | norm: 1.5853 dt: 539.59ms, tok/sec: 15181.89\n",
            "step  589, loss: 3.990244 | lr: 2.4000e-04 | norm: 1.5760 dt: 537.57ms, tok/sec: 15238.97\n",
            "step  590, loss: 3.967314 | lr: 2.4000e-04 | norm: 1.5968 dt: 541.33ms, tok/sec: 15133.12\n",
            "step  591, loss: 3.712160 | lr: 2.4000e-04 | norm: 1.2970 dt: 536.01ms, tok/sec: 15283.41\n",
            "step  592, loss: 3.907210 | lr: 2.4000e-04 | norm: 1.3263 dt: 539.83ms, tok/sec: 15175.16\n",
            "step  593, loss: 3.984241 | lr: 2.4000e-04 | norm: 1.5444 dt: 537.84ms, tok/sec: 15231.26\n",
            "step  594, loss: 3.896241 | lr: 2.4000e-04 | norm: 1.5459 dt: 538.37ms, tok/sec: 15216.20\n",
            "step  595, loss: 3.758727 | lr: 2.4000e-04 | norm: 1.3142 dt: 538.20ms, tok/sec: 15221.11\n",
            "step  596, loss: 3.925470 | lr: 2.4000e-04 | norm: 1.4184 dt: 540.91ms, tok/sec: 15144.82\n",
            "step  597, loss: 3.776744 | lr: 2.4000e-04 | norm: 1.4541 dt: 534.60ms, tok/sec: 15323.58\n",
            "step  598, loss: 4.065812 | lr: 2.4000e-04 | norm: 1.5774 dt: 539.57ms, tok/sec: 15182.54\n",
            "step  599, loss: 3.757769 | lr: 2.4000e-04 | norm: 1.4139 dt: 532.83ms, tok/sec: 15374.57\n",
            "Validation loss: 5.8183\n",
            "step  600, loss: 3.782308 | lr: 2.4000e-04 | norm: 1.2817 dt: 4121.12ms, tok/sec: 1987.81\n",
            "step  601, loss: 3.795416 | lr: 2.4000e-04 | norm: 1.2171 dt: 534.77ms, tok/sec: 15318.66\n",
            "step  602, loss: 4.138242 | lr: 2.4000e-04 | norm: 1.5982 dt: 538.04ms, tok/sec: 15225.60\n",
            "step  603, loss: 4.115911 | lr: 2.4000e-04 | norm: 1.4768 dt: 538.48ms, tok/sec: 15213.13\n",
            "step  604, loss: 4.333001 | lr: 2.4000e-04 | norm: 1.5670 dt: 536.90ms, tok/sec: 15257.85\n",
            "step  605, loss: 3.996368 | lr: 2.4000e-04 | norm: 1.3671 dt: 542.15ms, tok/sec: 15110.20\n",
            "step  606, loss: 4.129400 | lr: 2.4000e-04 | norm: 1.6416 dt: 536.02ms, tok/sec: 15282.92\n",
            "step  607, loss: 3.797525 | lr: 2.4000e-04 | norm: 1.8080 dt: 539.17ms, tok/sec: 15193.82\n",
            "step  608, loss: 3.767099 | lr: 2.4000e-04 | norm: 1.5568 dt: 538.36ms, tok/sec: 15216.50\n",
            "step  609, loss: 3.801092 | lr: 2.4000e-04 | norm: 1.9602 dt: 537.42ms, tok/sec: 15243.10\n",
            "step  610, loss: 3.650253 | lr: 2.4000e-04 | norm: 1.5367 dt: 538.82ms, tok/sec: 15203.71\n",
            "step  611, loss: 3.847152 | lr: 2.4000e-04 | norm: 1.5239 dt: 537.39ms, tok/sec: 15243.96\n",
            "step  612, loss: 3.911954 | lr: 2.4000e-04 | norm: 1.5512 dt: 536.11ms, tok/sec: 15280.37\n",
            "step  613, loss: 3.903099 | lr: 2.4000e-04 | norm: 1.4688 dt: 538.17ms, tok/sec: 15221.93\n",
            "step  614, loss: 3.627485 | lr: 2.4000e-04 | norm: 1.3154 dt: 535.61ms, tok/sec: 15294.61\n",
            "step  615, loss: 3.592935 | lr: 2.4000e-04 | norm: 1.3511 dt: 540.58ms, tok/sec: 15154.15\n",
            "step  616, loss: 3.700096 | lr: 2.4000e-04 | norm: 1.4706 dt: 534.84ms, tok/sec: 15316.69\n",
            "step  617, loss: 3.888116 | lr: 2.4000e-04 | norm: 1.5732 dt: 538.48ms, tok/sec: 15213.06\n",
            "step  618, loss: 3.728791 | lr: 2.4000e-04 | norm: 1.3560 dt: 534.49ms, tok/sec: 15326.89\n",
            "step  619, loss: 3.764956 | lr: 2.4000e-04 | norm: 1.3855 dt: 537.40ms, tok/sec: 15243.68\n",
            "step  620, loss: 3.502584 | lr: 2.4000e-04 | norm: 1.3712 dt: 537.39ms, tok/sec: 15243.96\n",
            "step  621, loss: 3.722392 | lr: 2.4000e-04 | norm: 1.3828 dt: 535.14ms, tok/sec: 15308.29\n",
            "step  622, loss: 3.601619 | lr: 2.4000e-04 | norm: 1.5370 dt: 535.87ms, tok/sec: 15287.26\n",
            "step  623, loss: 3.952254 | lr: 2.4000e-04 | norm: 1.6846 dt: 534.79ms, tok/sec: 15318.07\n",
            "step  624, loss: 4.026196 | lr: 2.4000e-04 | norm: 1.5080 dt: 539.86ms, tok/sec: 15174.22\n",
            "step  625, loss: 3.897607 | lr: 2.4000e-04 | norm: 1.6051 dt: 534.78ms, tok/sec: 15318.52\n",
            "step  626, loss: 3.875083 | lr: 2.4000e-04 | norm: 1.5755 dt: 542.09ms, tok/sec: 15111.85\n",
            "step  627, loss: 3.611475 | lr: 2.4000e-04 | norm: 1.3057 dt: 535.52ms, tok/sec: 15297.20\n",
            "step  628, loss: 3.799040 | lr: 2.4000e-04 | norm: 1.3577 dt: 537.61ms, tok/sec: 15237.93\n",
            "step  629, loss: 3.877186 | lr: 2.4000e-04 | norm: 1.4190 dt: 536.54ms, tok/sec: 15268.31\n",
            "step  630, loss: 3.770910 | lr: 2.4000e-04 | norm: 1.3755 dt: 539.14ms, tok/sec: 15194.55\n",
            "step  631, loss: 3.640618 | lr: 2.4000e-04 | norm: 1.3389 dt: 534.86ms, tok/sec: 15316.16\n",
            "step  632, loss: 3.832638 | lr: 2.4000e-04 | norm: 1.6856 dt: 538.74ms, tok/sec: 15205.95\n",
            "step  633, loss: 3.691758 | lr: 2.4000e-04 | norm: 1.6285 dt: 536.15ms, tok/sec: 15279.16\n",
            "step  634, loss: 3.971373 | lr: 2.4000e-04 | norm: 1.6497 dt: 536.55ms, tok/sec: 15267.79\n",
            "step  635, loss: 3.643816 | lr: 2.4000e-04 | norm: 1.3374 dt: 533.29ms, tok/sec: 15361.35\n",
            "step  636, loss: 3.673171 | lr: 2.4000e-04 | norm: 1.3429 dt: 539.64ms, tok/sec: 15180.53\n",
            "step  637, loss: 3.694911 | lr: 2.4000e-04 | norm: 1.7556 dt: 534.28ms, tok/sec: 15332.74\n",
            "step  638, loss: 4.049328 | lr: 2.4000e-04 | norm: 2.0284 dt: 539.32ms, tok/sec: 15189.43\n",
            "step  639, loss: 4.033930 | lr: 2.4000e-04 | norm: 1.6720 dt: 532.67ms, tok/sec: 15379.13\n",
            "step  640, loss: 4.231008 | lr: 2.4000e-04 | norm: 1.7421 dt: 538.92ms, tok/sec: 15200.90\n",
            "step  641, loss: 3.906704 | lr: 2.4000e-04 | norm: 1.5804 dt: 533.96ms, tok/sec: 15341.94\n",
            "step  642, loss: 4.048334 | lr: 2.4000e-04 | norm: 1.5057 dt: 543.74ms, tok/sec: 15065.91\n",
            "step  643, loss: 3.727962 | lr: 2.4000e-04 | norm: 1.5966 dt: 534.36ms, tok/sec: 15330.40\n",
            "step  644, loss: 3.673931 | lr: 2.4000e-04 | norm: 1.4632 dt: 542.15ms, tok/sec: 15110.24\n",
            "step  645, loss: 3.720465 | lr: 2.4000e-04 | norm: 1.8205 dt: 536.97ms, tok/sec: 15255.89\n",
            "step  646, loss: 3.596602 | lr: 2.4000e-04 | norm: 1.7341 dt: 535.96ms, tok/sec: 15284.78\n",
            "step  647, loss: 3.798332 | lr: 2.4000e-04 | norm: 1.7265 dt: 536.59ms, tok/sec: 15266.75\n",
            "step  648, loss: 3.860365 | lr: 2.4000e-04 | norm: 1.8457 dt: 540.47ms, tok/sec: 15157.28\n",
            "step  649, loss: 3.834251 | lr: 2.4000e-04 | norm: 1.7547 dt: 536.02ms, tok/sec: 15282.87\n",
            "step  650, loss: 3.562491 | lr: 2.4000e-04 | norm: 1.8668 dt: 542.22ms, tok/sec: 15108.37\n",
            "step  651, loss: 3.521064 | lr: 2.4000e-04 | norm: 1.7550 dt: 532.53ms, tok/sec: 15383.07\n",
            "step  652, loss: 3.625261 | lr: 2.4000e-04 | norm: 1.6417 dt: 540.04ms, tok/sec: 15169.35\n",
            "step  653, loss: 3.812619 | lr: 2.4000e-04 | norm: 1.7680 dt: 536.74ms, tok/sec: 15262.40\n",
            "step  654, loss: 3.634019 | lr: 2.4000e-04 | norm: 1.5270 dt: 539.32ms, tok/sec: 15189.61\n",
            "step  655, loss: 3.670852 | lr: 2.4000e-04 | norm: 1.6105 dt: 538.06ms, tok/sec: 15225.16\n",
            "step  656, loss: 3.428534 | lr: 2.4000e-04 | norm: 1.6446 dt: 536.76ms, tok/sec: 15261.91\n",
            "step  657, loss: 3.644112 | lr: 2.4000e-04 | norm: 1.4869 dt: 535.77ms, tok/sec: 15290.05\n",
            "step  658, loss: 3.511161 | lr: 2.4000e-04 | norm: 1.3729 dt: 536.25ms, tok/sec: 15276.37\n",
            "step  659, loss: 3.860625 | lr: 2.4000e-04 | norm: 1.5620 dt: 534.10ms, tok/sec: 15337.91\n",
            "step  660, loss: 3.924226 | lr: 2.4000e-04 | norm: 1.7203 dt: 541.49ms, tok/sec: 15128.68\n",
            "step  661, loss: 3.807353 | lr: 2.4000e-04 | norm: 1.7012 dt: 533.12ms, tok/sec: 15366.26\n",
            "step  662, loss: 3.759043 | lr: 2.4000e-04 | norm: 1.4751 dt: 539.34ms, tok/sec: 15188.97\n",
            "step  663, loss: 3.507082 | lr: 2.4000e-04 | norm: 1.4469 dt: 534.66ms, tok/sec: 15321.92\n",
            "step  664, loss: 3.707149 | lr: 2.4000e-04 | norm: 1.6661 dt: 539.29ms, tok/sec: 15190.42\n",
            "step  665, loss: 3.786947 | lr: 2.4000e-04 | norm: 1.5636 dt: 535.97ms, tok/sec: 15284.47\n",
            "step  666, loss: 3.668344 | lr: 2.4000e-04 | norm: 1.5170 dt: 535.27ms, tok/sec: 15304.53\n",
            "step  667, loss: 3.576632 | lr: 2.4000e-04 | norm: 1.5781 dt: 537.11ms, tok/sec: 15251.88\n",
            "step  668, loss: 3.759146 | lr: 2.4000e-04 | norm: 1.6415 dt: 539.95ms, tok/sec: 15171.64\n",
            "step  669, loss: 3.608775 | lr: 2.4000e-04 | norm: 1.5362 dt: 537.73ms, tok/sec: 15234.33\n",
            "step  670, loss: 3.884921 | lr: 2.4000e-04 | norm: 1.8390 dt: 538.40ms, tok/sec: 15215.31\n",
            "step  671, loss: 3.573041 | lr: 2.4000e-04 | norm: 1.7095 dt: 535.75ms, tok/sec: 15290.61\n",
            "step  672, loss: 3.605764 | lr: 2.4000e-04 | norm: 1.7096 dt: 541.56ms, tok/sec: 15126.60\n",
            "step  673, loss: 3.629905 | lr: 2.4000e-04 | norm: 1.7019 dt: 535.39ms, tok/sec: 15301.03\n",
            "step  674, loss: 3.974943 | lr: 2.4000e-04 | norm: 1.9975 dt: 536.97ms, tok/sec: 15255.92\n",
            "step  675, loss: 3.949943 | lr: 2.4000e-04 | norm: 1.8242 dt: 535.20ms, tok/sec: 15306.43\n",
            "step  676, loss: 4.161187 | lr: 2.4000e-04 | norm: 2.3493 dt: 536.71ms, tok/sec: 15263.44\n",
            "step  677, loss: 3.825774 | lr: 2.4000e-04 | norm: 1.9425 dt: 539.01ms, tok/sec: 15198.35\n",
            "step  678, loss: 3.951961 | lr: 2.4000e-04 | norm: 1.7725 dt: 534.22ms, tok/sec: 15334.42\n",
            "step  679, loss: 3.627393 | lr: 2.4000e-04 | norm: 1.6532 dt: 537.83ms, tok/sec: 15231.61\n",
            "step  680, loss: 3.599930 | lr: 2.4000e-04 | norm: 1.5783 dt: 534.87ms, tok/sec: 15315.93\n",
            "step  681, loss: 3.661671 | lr: 2.4000e-04 | norm: 1.8878 dt: 539.98ms, tok/sec: 15170.82\n",
            "step  682, loss: 3.512296 | lr: 2.4000e-04 | norm: 1.6857 dt: 534.44ms, tok/sec: 15328.30\n",
            "step  683, loss: 3.714883 | lr: 2.4000e-04 | norm: 1.7101 dt: 539.42ms, tok/sec: 15186.77\n",
            "step  684, loss: 3.799679 | lr: 2.4000e-04 | norm: 1.9312 dt: 536.53ms, tok/sec: 15268.59\n",
            "step  685, loss: 3.775902 | lr: 2.4000e-04 | norm: 1.9764 dt: 537.78ms, tok/sec: 15232.99\n",
            "step  686, loss: 3.513652 | lr: 2.4000e-04 | norm: 1.7283 dt: 535.70ms, tok/sec: 15292.27\n",
            "step  687, loss: 3.480706 | lr: 2.4000e-04 | norm: 2.0360 dt: 534.19ms, tok/sec: 15335.51\n",
            "step  688, loss: 3.579462 | lr: 2.4000e-04 | norm: 2.0622 dt: 538.08ms, tok/sec: 15224.50\n",
            "step  689, loss: 3.774512 | lr: 2.4000e-04 | norm: 2.0078 dt: 535.46ms, tok/sec: 15299.08\n",
            "step  690, loss: 3.574944 | lr: 2.4000e-04 | norm: 1.6465 dt: 539.32ms, tok/sec: 15189.45\n",
            "step  691, loss: 3.606903 | lr: 2.4000e-04 | norm: 1.6522 dt: 533.22ms, tok/sec: 15363.22\n",
            "step  692, loss: 3.381451 | lr: 2.4000e-04 | norm: 1.7422 dt: 542.63ms, tok/sec: 15096.77\n",
            "step  693, loss: 3.572779 | lr: 2.4000e-04 | norm: 1.8019 dt: 536.52ms, tok/sec: 15268.66\n",
            "step  694, loss: 3.435663 | lr: 2.4000e-04 | norm: 1.7394 dt: 538.30ms, tok/sec: 15218.17\n",
            "step  695, loss: 3.767079 | lr: 2.4000e-04 | norm: 1.6717 dt: 537.26ms, tok/sec: 15247.83\n",
            "step  696, loss: 3.837578 | lr: 2.4000e-04 | norm: 1.6915 dt: 538.30ms, tok/sec: 15218.27\n",
            "step  697, loss: 3.728861 | lr: 2.4000e-04 | norm: 1.6930 dt: 536.73ms, tok/sec: 15262.82\n",
            "step  698, loss: 3.681222 | lr: 2.4000e-04 | norm: 1.6724 dt: 536.43ms, tok/sec: 15271.23\n",
            "step  699, loss: 3.434346 | lr: 2.4000e-04 | norm: 1.6182 dt: 535.75ms, tok/sec: 15290.85\n",
            "Validation loss: 5.7367\n",
            "step  700, loss: 3.650190 | lr: 2.4000e-04 | norm: 1.8867 dt: 4103.40ms, tok/sec: 1996.39\n",
            "step  701, loss: 3.731732 | lr: 2.4000e-04 | norm: 1.9308 dt: 531.94ms, tok/sec: 15400.30\n",
            "step  702, loss: 3.608715 | lr: 2.4000e-04 | norm: 1.8805 dt: 540.45ms, tok/sec: 15157.67\n",
            "step  703, loss: 3.534009 | lr: 2.4000e-04 | norm: 1.9409 dt: 534.74ms, tok/sec: 15319.57\n",
            "step  704, loss: 3.688495 | lr: 2.4000e-04 | norm: 1.7598 dt: 535.75ms, tok/sec: 15290.60\n",
            "step  705, loss: 3.541811 | lr: 2.4000e-04 | norm: 1.6435 dt: 534.11ms, tok/sec: 15337.80\n",
            "step  706, loss: 3.826539 | lr: 2.4000e-04 | norm: 1.7758 dt: 538.18ms, tok/sec: 15221.53\n",
            "step  707, loss: 3.501920 | lr: 2.4000e-04 | norm: 1.5358 dt: 532.77ms, tok/sec: 15376.12\n",
            "step  708, loss: 3.519930 | lr: 2.4000e-04 | norm: 1.4376 dt: 536.40ms, tok/sec: 15272.26\n",
            "step  709, loss: 3.546438 | lr: 2.4000e-04 | norm: 1.6956 dt: 536.82ms, tok/sec: 15260.29\n",
            "step  710, loss: 3.901350 | lr: 2.4000e-04 | norm: 2.1482 dt: 537.68ms, tok/sec: 15235.84\n",
            "step  711, loss: 3.891363 | lr: 2.4000e-04 | norm: 1.9502 dt: 534.11ms, tok/sec: 15337.75\n",
            "step  712, loss: 4.102791 | lr: 2.4000e-04 | norm: 2.1690 dt: 532.51ms, tok/sec: 15383.66\n",
            "step  713, loss: 3.754105 | lr: 2.4000e-04 | norm: 1.8481 dt: 538.00ms, tok/sec: 15226.74\n",
            "step  714, loss: 3.900604 | lr: 2.4000e-04 | norm: 1.8379 dt: 535.78ms, tok/sec: 15289.74\n",
            "step  715, loss: 3.550694 | lr: 2.4000e-04 | norm: 1.8539 dt: 536.41ms, tok/sec: 15271.82\n",
            "step  716, loss: 3.524614 | lr: 2.4000e-04 | norm: 1.6646 dt: 534.94ms, tok/sec: 15314.00\n",
            "step  717, loss: 3.557504 | lr: 2.4000e-04 | norm: 1.8304 dt: 536.02ms, tok/sec: 15282.95\n",
            "step  718, loss: 3.405435 | lr: 2.4000e-04 | norm: 1.5009 dt: 532.03ms, tok/sec: 15397.60\n",
            "step  719, loss: 3.615345 | lr: 2.4000e-04 | norm: 1.7796 dt: 539.25ms, tok/sec: 15191.54\n",
            "step  720, loss: 3.739956 | lr: 2.4000e-04 | norm: 2.1813 dt: 534.77ms, tok/sec: 15318.82\n",
            "step  721, loss: 3.723290 | lr: 2.4000e-04 | norm: 2.1190 dt: 537.27ms, tok/sec: 15247.59\n",
            "step  722, loss: 3.433713 | lr: 2.4000e-04 | norm: 1.6322 dt: 533.76ms, tok/sec: 15347.80\n",
            "step  723, loss: 3.398996 | lr: 2.4000e-04 | norm: 1.5771 dt: 537.18ms, tok/sec: 15249.99\n",
            "step  724, loss: 3.492285 | lr: 2.4000e-04 | norm: 1.7263 dt: 534.72ms, tok/sec: 15320.15\n",
            "step  725, loss: 3.669163 | lr: 2.4000e-04 | norm: 1.8666 dt: 534.53ms, tok/sec: 15325.53\n",
            "step  726, loss: 3.470811 | lr: 2.4000e-04 | norm: 1.6264 dt: 536.21ms, tok/sec: 15277.67\n",
            "step  727, loss: 3.494665 | lr: 2.4000e-04 | norm: 1.4386 dt: 532.36ms, tok/sec: 15388.12\n",
            "step  728, loss: 3.289984 | lr: 2.4000e-04 | norm: 1.4127 dt: 537.19ms, tok/sec: 15249.77\n",
            "step  729, loss: 3.489320 | lr: 2.4000e-04 | norm: 1.6105 dt: 534.00ms, tok/sec: 15340.74\n",
            "step  730, loss: 3.355197 | lr: 2.4000e-04 | norm: 1.6118 dt: 537.22ms, tok/sec: 15248.89\n",
            "step  731, loss: 3.676623 | lr: 2.4000e-04 | norm: 1.7165 dt: 536.35ms, tok/sec: 15273.52\n",
            "step  732, loss: 3.738131 | lr: 2.4000e-04 | norm: 1.6765 dt: 537.62ms, tok/sec: 15237.53\n",
            "step  733, loss: 3.637877 | lr: 2.4000e-04 | norm: 1.7803 dt: 533.61ms, tok/sec: 15352.06\n",
            "step  734, loss: 3.601697 | lr: 2.4000e-04 | norm: 2.1021 dt: 536.68ms, tok/sec: 15264.10\n",
            "step  735, loss: 3.385123 | lr: 2.4000e-04 | norm: 2.0500 dt: 531.90ms, tok/sec: 15401.46\n",
            "step  736, loss: 3.585414 | lr: 2.4000e-04 | norm: 1.9656 dt: 537.56ms, tok/sec: 15239.18\n",
            "step  737, loss: 3.636607 | lr: 2.4000e-04 | norm: 1.6691 dt: 533.79ms, tok/sec: 15346.79\n",
            "step  738, loss: 3.508569 | lr: 2.4000e-04 | norm: 1.8400 dt: 533.89ms, tok/sec: 15344.07\n",
            "step  739, loss: 3.440929 | lr: 2.4000e-04 | norm: 1.7240 dt: 532.54ms, tok/sec: 15382.78\n",
            "step  740, loss: 3.609009 | lr: 2.4000e-04 | norm: 1.9455 dt: 531.84ms, tok/sec: 15403.18\n",
            "step  741, loss: 3.463482 | lr: 2.4000e-04 | norm: 1.9403 dt: 536.27ms, tok/sec: 15275.91\n",
            "step  742, loss: 3.741765 | lr: 2.4000e-04 | norm: 2.0326 dt: 532.90ms, tok/sec: 15372.36\n",
            "step  743, loss: 3.412528 | lr: 2.4000e-04 | norm: 1.4875 dt: 534.10ms, tok/sec: 15338.02\n",
            "step  744, loss: 3.428050 | lr: 2.4000e-04 | norm: 1.4322 dt: 534.46ms, tok/sec: 15327.74\n",
            "step  745, loss: 3.439466 | lr: 2.4000e-04 | norm: 1.3767 dt: 532.11ms, tok/sec: 15395.34\n",
            "step  746, loss: 3.765226 | lr: 2.4000e-04 | norm: 1.4612 dt: 536.43ms, tok/sec: 15271.34\n",
            "step  747, loss: 3.742886 | lr: 2.4000e-04 | norm: 1.5678 dt: 534.14ms, tok/sec: 15336.94\n",
            "step  748, loss: 3.954166 | lr: 2.4000e-04 | norm: 2.0705 dt: 540.23ms, tok/sec: 15164.02\n",
            "step  749, loss: 3.622547 | lr: 2.4000e-04 | norm: 1.8389 dt: 533.54ms, tok/sec: 15353.96\n",
            "step  750, loss: 3.780921 | lr: 2.4000e-04 | norm: 1.9354 dt: 535.21ms, tok/sec: 15306.23\n",
            "step  751, loss: 3.459997 | lr: 2.4000e-04 | norm: 1.8324 dt: 536.07ms, tok/sec: 15281.71\n",
            "step  752, loss: 3.450061 | lr: 2.4000e-04 | norm: 1.6132 dt: 536.31ms, tok/sec: 15274.79\n",
            "step  753, loss: 3.448955 | lr: 2.4000e-04 | norm: 1.7532 dt: 536.24ms, tok/sec: 15276.60\n",
            "step  754, loss: 3.300591 | lr: 2.4000e-04 | norm: 1.5613 dt: 536.51ms, tok/sec: 15269.17\n",
            "step  755, loss: 3.519269 | lr: 2.4000e-04 | norm: 1.7836 dt: 535.60ms, tok/sec: 15295.06\n",
            "step  756, loss: 3.618384 | lr: 2.4000e-04 | norm: 1.7875 dt: 536.35ms, tok/sec: 15273.64\n",
            "step  757, loss: 3.593282 | lr: 2.4000e-04 | norm: 1.7854 dt: 534.58ms, tok/sec: 15324.11\n",
            "step  758, loss: 3.308575 | lr: 2.4000e-04 | norm: 1.5813 dt: 535.35ms, tok/sec: 15302.01\n",
            "step  759, loss: 3.272696 | lr: 2.4000e-04 | norm: 1.6715 dt: 534.08ms, tok/sec: 15338.59\n",
            "step  760, loss: 3.378017 | lr: 2.4000e-04 | norm: 1.7361 dt: 537.68ms, tok/sec: 15235.88\n",
            "step  761, loss: 3.542564 | lr: 2.4000e-04 | norm: 1.8008 dt: 539.49ms, tok/sec: 15184.79\n",
            "step  762, loss: 3.390129 | lr: 2.4000e-04 | norm: 1.6805 dt: 534.87ms, tok/sec: 15315.95\n",
            "step  763, loss: 3.377729 | lr: 2.4000e-04 | norm: 1.5498 dt: 533.94ms, tok/sec: 15342.59\n",
            "step  764, loss: 3.178674 | lr: 2.4000e-04 | norm: 1.4774 dt: 534.52ms, tok/sec: 15325.90\n",
            "step  765, loss: 3.378994 | lr: 2.4000e-04 | norm: 1.5038 dt: 535.78ms, tok/sec: 15289.74\n",
            "step  766, loss: 3.237306 | lr: 2.4000e-04 | norm: 1.4676 dt: 534.84ms, tok/sec: 15316.82\n",
            "step  767, loss: 3.553888 | lr: 2.4000e-04 | norm: 1.5908 dt: 537.54ms, tok/sec: 15239.86\n",
            "step  768, loss: 3.637022 | lr: 2.4000e-04 | norm: 1.6884 dt: 537.46ms, tok/sec: 15242.02\n",
            "step  769, loss: 3.505715 | lr: 2.4000e-04 | norm: 1.4896 dt: 535.48ms, tok/sec: 15298.37\n",
            "step  770, loss: 3.476996 | lr: 2.4000e-04 | norm: 1.4601 dt: 533.23ms, tok/sec: 15362.84\n",
            "step  771, loss: 3.247540 | lr: 2.4000e-04 | norm: 1.4929 dt: 539.36ms, tok/sec: 15188.28\n",
            "step  772, loss: 3.464918 | lr: 2.4000e-04 | norm: 1.7175 dt: 537.51ms, tok/sec: 15240.56\n",
            "step  773, loss: 3.519001 | lr: 2.4000e-04 | norm: 1.8669 dt: 531.01ms, tok/sec: 15427.08\n",
            "step  774, loss: 3.389507 | lr: 2.4000e-04 | norm: 1.6931 dt: 536.86ms, tok/sec: 15259.19\n",
            "step  775, loss: 3.317772 | lr: 2.4000e-04 | norm: 1.5127 dt: 534.49ms, tok/sec: 15326.72\n",
            "step  776, loss: 3.476440 | lr: 2.4000e-04 | norm: 1.5575 dt: 537.30ms, tok/sec: 15246.72\n",
            "step  777, loss: 3.350580 | lr: 2.4000e-04 | norm: 1.7653 dt: 535.06ms, tok/sec: 15310.35\n",
            "step  778, loss: 3.629697 | lr: 2.4000e-04 | norm: 1.9457 dt: 536.63ms, tok/sec: 15265.78\n",
            "step  779, loss: 3.286732 | lr: 2.4000e-04 | norm: 1.6179 dt: 533.52ms, tok/sec: 15354.65\n",
            "step  780, loss: 3.307014 | lr: 2.4000e-04 | norm: 1.5258 dt: 535.29ms, tok/sec: 15303.81\n",
            "step  781, loss: 3.306549 | lr: 2.4000e-04 | norm: 1.4571 dt: 533.45ms, tok/sec: 15356.70\n",
            "step  782, loss: 3.653471 | lr: 2.4000e-04 | norm: 1.8465 dt: 538.46ms, tok/sec: 15213.80\n",
            "step  783, loss: 3.629313 | lr: 2.4000e-04 | norm: 1.5998 dt: 531.05ms, tok/sec: 15426.09\n",
            "step  784, loss: 3.833142 | lr: 2.4000e-04 | norm: 1.5989 dt: 535.74ms, tok/sec: 15290.97\n",
            "step  785, loss: 3.499968 | lr: 2.4000e-04 | norm: 1.5625 dt: 536.52ms, tok/sec: 15268.68\n",
            "step  786, loss: 3.648846 | lr: 2.4000e-04 | norm: 1.8524 dt: 533.96ms, tok/sec: 15341.89\n",
            "step  787, loss: 3.334062 | lr: 2.4000e-04 | norm: 1.8430 dt: 538.26ms, tok/sec: 15219.28\n",
            "step  788, loss: 3.328275 | lr: 2.4000e-04 | norm: 1.6141 dt: 536.57ms, tok/sec: 15267.26\n",
            "step  789, loss: 3.331090 | lr: 2.4000e-04 | norm: 1.8861 dt: 536.42ms, tok/sec: 15271.69\n",
            "step  790, loss: 3.186982 | lr: 2.4000e-04 | norm: 1.6212 dt: 533.63ms, tok/sec: 15351.37\n",
            "step  791, loss: 3.409955 | lr: 2.4000e-04 | norm: 1.8490 dt: 535.78ms, tok/sec: 15289.97\n",
            "step  792, loss: 3.519523 | lr: 2.4000e-04 | norm: 1.8507 dt: 532.67ms, tok/sec: 15379.17\n",
            "step  793, loss: 3.518126 | lr: 2.4000e-04 | norm: 2.0299 dt: 535.76ms, tok/sec: 15290.51\n",
            "step  794, loss: 3.246465 | lr: 2.4000e-04 | norm: 1.8839 dt: 532.67ms, tok/sec: 15379.26\n",
            "step  795, loss: 3.209698 | lr: 2.4000e-04 | norm: 1.8666 dt: 536.80ms, tok/sec: 15260.94\n",
            "step  796, loss: 3.284053 | lr: 2.4000e-04 | norm: 1.8763 dt: 534.57ms, tok/sec: 15324.54\n",
            "step  797, loss: 3.452725 | lr: 2.4000e-04 | norm: 2.1052 dt: 538.92ms, tok/sec: 15200.66\n",
            "step  798, loss: 3.270361 | lr: 2.4000e-04 | norm: 1.9067 dt: 532.53ms, tok/sec: 15383.03\n",
            "step  799, loss: 3.269444 | lr: 2.4000e-04 | norm: 1.7102 dt: 539.01ms, tok/sec: 15198.15\n",
            "Validation loss: 5.8428\n",
            "step  800, loss: 3.075499 | lr: 2.4000e-04 | norm: 1.5752 dt: 4081.79ms, tok/sec: 2006.96\n",
            "step  801, loss: 3.269430 | lr: 2.4000e-04 | norm: 1.6756 dt: 531.46ms, tok/sec: 15414.04\n",
            "step  802, loss: 3.149707 | lr: 2.4000e-04 | norm: 1.6476 dt: 536.63ms, tok/sec: 15265.64\n",
            "step  803, loss: 3.449231 | lr: 2.4000e-04 | norm: 1.7796 dt: 531.95ms, tok/sec: 15399.81\n",
            "step  804, loss: 3.523523 | lr: 2.4000e-04 | norm: 1.8270 dt: 535.27ms, tok/sec: 15304.32\n",
            "step  805, loss: 3.390613 | lr: 2.4000e-04 | norm: 1.7912 dt: 532.24ms, tok/sec: 15391.54\n",
            "step  806, loss: 3.354621 | lr: 2.4000e-04 | norm: 1.7327 dt: 533.40ms, tok/sec: 15358.13\n",
            "step  807, loss: 3.130730 | lr: 2.4000e-04 | norm: 1.6440 dt: 534.01ms, tok/sec: 15340.61\n",
            "step  808, loss: 3.366457 | lr: 2.4000e-04 | norm: 1.8427 dt: 536.10ms, tok/sec: 15280.82\n",
            "step  809, loss: 3.437067 | lr: 2.4000e-04 | norm: 1.9326 dt: 531.61ms, tok/sec: 15409.72\n",
            "step  810, loss: 3.289066 | lr: 2.4000e-04 | norm: 1.7464 dt: 535.10ms, tok/sec: 15309.19\n",
            "step  811, loss: 3.207412 | lr: 2.4000e-04 | norm: 1.5785 dt: 533.26ms, tok/sec: 15362.14\n",
            "step  812, loss: 3.366164 | lr: 2.4000e-04 | norm: 1.9428 dt: 534.55ms, tok/sec: 15325.14\n",
            "step  813, loss: 3.244959 | lr: 2.4000e-04 | norm: 1.8363 dt: 535.95ms, tok/sec: 15285.02\n",
            "step  814, loss: 3.516312 | lr: 2.4000e-04 | norm: 1.9360 dt: 533.63ms, tok/sec: 15351.40\n",
            "step  815, loss: 3.180792 | lr: 2.4000e-04 | norm: 1.6168 dt: 533.47ms, tok/sec: 15355.98\n",
            "step  816, loss: 3.221299 | lr: 2.4000e-04 | norm: 1.8481 dt: 536.37ms, tok/sec: 15272.90\n",
            "step  817, loss: 3.208004 | lr: 2.4000e-04 | norm: 1.8357 dt: 533.87ms, tok/sec: 15344.53\n",
            "step  818, loss: 3.560504 | lr: 2.4000e-04 | norm: 2.1449 dt: 536.50ms, tok/sec: 15269.22\n",
            "step  819, loss: 3.528414 | lr: 2.4000e-04 | norm: 1.9349 dt: 530.74ms, tok/sec: 15435.02\n",
            "step  820, loss: 3.733212 | lr: 2.4000e-04 | norm: 2.2937 dt: 539.37ms, tok/sec: 15188.18\n",
            "step  821, loss: 3.403175 | lr: 2.4000e-04 | norm: 1.9513 dt: 530.13ms, tok/sec: 15452.89\n",
            "step  822, loss: 3.524194 | lr: 2.4000e-04 | norm: 1.7663 dt: 539.56ms, tok/sec: 15182.79\n",
            "step  823, loss: 3.250580 | lr: 2.4000e-04 | norm: 2.0934 dt: 532.38ms, tok/sec: 15387.59\n",
            "step  824, loss: 3.235558 | lr: 2.4000e-04 | norm: 1.8284 dt: 534.84ms, tok/sec: 15316.67\n",
            "step  825, loss: 3.262328 | lr: 2.4000e-04 | norm: 2.1670 dt: 536.87ms, tok/sec: 15258.81\n",
            "step  826, loss: 3.095335 | lr: 2.4000e-04 | norm: 1.6608 dt: 532.97ms, tok/sec: 15370.52\n",
            "step  827, loss: 3.321690 | lr: 2.4000e-04 | norm: 1.8158 dt: 539.37ms, tok/sec: 15188.14\n",
            "step  828, loss: 3.412247 | lr: 2.4000e-04 | norm: 1.9409 dt: 535.02ms, tok/sec: 15311.69\n",
            "step  829, loss: 3.409854 | lr: 2.4000e-04 | norm: 2.0890 dt: 533.53ms, tok/sec: 15354.25\n",
            "step  830, loss: 3.123226 | lr: 2.4000e-04 | norm: 1.7709 dt: 534.75ms, tok/sec: 15319.22\n",
            "step  831, loss: 3.094355 | lr: 2.4000e-04 | norm: 1.8086 dt: 537.31ms, tok/sec: 15246.38\n",
            "step  832, loss: 3.185887 | lr: 2.4000e-04 | norm: 2.0120 dt: 534.50ms, tok/sec: 15326.57\n",
            "step  833, loss: 3.385151 | lr: 2.4000e-04 | norm: 2.0609 dt: 535.53ms, tok/sec: 15297.01\n",
            "step  834, loss: 3.201736 | lr: 2.4000e-04 | norm: 1.9668 dt: 535.13ms, tok/sec: 15308.55\n",
            "step  835, loss: 3.199804 | lr: 2.4000e-04 | norm: 1.9278 dt: 536.85ms, tok/sec: 15259.32\n",
            "step  836, loss: 2.985284 | lr: 2.4000e-04 | norm: 1.8848 dt: 532.27ms, tok/sec: 15390.76\n",
            "step  837, loss: 3.166115 | lr: 2.4000e-04 | norm: 1.8964 dt: 536.44ms, tok/sec: 15271.17\n",
            "step  838, loss: 3.034799 | lr: 2.4000e-04 | norm: 1.8250 dt: 535.15ms, tok/sec: 15307.77\n",
            "step  839, loss: 3.343333 | lr: 2.4000e-04 | norm: 1.9999 dt: 536.12ms, tok/sec: 15280.05\n",
            "step  840, loss: 3.441121 | lr: 2.4000e-04 | norm: 2.0623 dt: 536.42ms, tok/sec: 15271.76\n",
            "step  841, loss: 3.294842 | lr: 2.4000e-04 | norm: 1.9997 dt: 537.53ms, tok/sec: 15240.01\n",
            "step  842, loss: 3.271432 | lr: 2.4000e-04 | norm: 2.0728 dt: 537.54ms, tok/sec: 15239.74\n",
            "step  843, loss: 3.058217 | lr: 2.4000e-04 | norm: 1.9859 dt: 536.23ms, tok/sec: 15276.99\n",
            "step  844, loss: 3.277262 | lr: 2.4000e-04 | norm: 2.0805 dt: 540.62ms, tok/sec: 15152.84\n",
            "step  845, loss: 3.341599 | lr: 2.4000e-04 | norm: 1.9661 dt: 532.67ms, tok/sec: 15379.21\n",
            "step  846, loss: 3.212276 | lr: 2.4000e-04 | norm: 2.0284 dt: 536.36ms, tok/sec: 15273.30\n",
            "step  847, loss: 3.118679 | lr: 2.4000e-04 | norm: 1.9107 dt: 535.63ms, tok/sec: 15294.05\n",
            "step  848, loss: 3.302744 | lr: 2.4000e-04 | norm: 2.1140 dt: 539.72ms, tok/sec: 15178.37\n",
            "step  849, loss: 3.190094 | lr: 2.4000e-04 | norm: 2.2957 dt: 534.51ms, tok/sec: 15326.31\n",
            "step  850, loss: 3.458451 | lr: 2.4000e-04 | norm: 2.5584 dt: 539.99ms, tok/sec: 15170.58\n",
            "step  851, loss: 3.101660 | lr: 2.4000e-04 | norm: 1.8908 dt: 532.41ms, tok/sec: 15386.60\n",
            "step  852, loss: 3.133221 | lr: 2.4000e-04 | norm: 1.7380 dt: 537.97ms, tok/sec: 15227.57\n",
            "step  853, loss: 3.138058 | lr: 2.4000e-04 | norm: 1.9304 dt: 535.42ms, tok/sec: 15300.04\n",
            "step  854, loss: 3.477071 | lr: 2.4000e-04 | norm: 2.1984 dt: 538.56ms, tok/sec: 15210.85\n",
            "step  855, loss: 3.445085 | lr: 2.4000e-04 | norm: 1.9879 dt: 534.55ms, tok/sec: 15325.14\n",
            "step  856, loss: 3.650531 | lr: 2.4000e-04 | norm: 2.4304 dt: 536.69ms, tok/sec: 15263.88\n",
            "step  857, loss: 3.358479 | lr: 2.4000e-04 | norm: 2.3804 dt: 535.94ms, tok/sec: 15285.26\n",
            "step  858, loss: 3.473674 | lr: 2.4000e-04 | norm: 2.2329 dt: 535.31ms, tok/sec: 15303.29\n",
            "step  859, loss: 3.202955 | lr: 2.4000e-04 | norm: 2.0760 dt: 535.99ms, tok/sec: 15283.80\n",
            "step  860, loss: 3.144012 | lr: 2.4000e-04 | norm: 1.8031 dt: 535.17ms, tok/sec: 15307.16\n",
            "step  861, loss: 3.159741 | lr: 2.4000e-04 | norm: 2.1818 dt: 539.81ms, tok/sec: 15175.57\n",
            "step  862, loss: 2.999167 | lr: 2.4000e-04 | norm: 1.9259 dt: 532.96ms, tok/sec: 15370.84\n",
            "step  863, loss: 3.221432 | lr: 2.4000e-04 | norm: 1.9917 dt: 540.90ms, tok/sec: 15145.03\n",
            "step  864, loss: 3.319208 | lr: 2.4000e-04 | norm: 2.0761 dt: 532.83ms, tok/sec: 15374.62\n",
            "step  865, loss: 3.317027 | lr: 2.4000e-04 | norm: 2.0677 dt: 537.87ms, tok/sec: 15230.34\n",
            "step  866, loss: 3.006505 | lr: 2.4000e-04 | norm: 1.7045 dt: 536.67ms, tok/sec: 15264.45\n",
            "step  867, loss: 2.973208 | lr: 2.4000e-04 | norm: 1.6750 dt: 535.95ms, tok/sec: 15284.97\n",
            "step  868, loss: 3.052481 | lr: 2.4000e-04 | norm: 1.7891 dt: 534.88ms, tok/sec: 15315.48\n",
            "step  869, loss: 3.252035 | lr: 2.4000e-04 | norm: 1.9761 dt: 536.82ms, tok/sec: 15260.16\n",
            "step  870, loss: 3.104444 | lr: 2.4000e-04 | norm: 1.9655 dt: 534.44ms, tok/sec: 15328.08\n",
            "step  871, loss: 3.100591 | lr: 2.4000e-04 | norm: 1.9196 dt: 534.30ms, tok/sec: 15332.17\n",
            "step  872, loss: 2.899333 | lr: 2.4000e-04 | norm: 1.7708 dt: 536.84ms, tok/sec: 15259.60\n",
            "step  873, loss: 3.074525 | lr: 2.4000e-04 | norm: 1.7736 dt: 535.46ms, tok/sec: 15298.91\n",
            "step  874, loss: 2.958283 | lr: 2.4000e-04 | norm: 1.8937 dt: 537.29ms, tok/sec: 15247.02\n",
            "step  875, loss: 3.270098 | lr: 2.4000e-04 | norm: 2.2175 dt: 534.59ms, tok/sec: 15323.82\n",
            "step  876, loss: 3.346076 | lr: 2.4000e-04 | norm: 2.1925 dt: 536.09ms, tok/sec: 15280.97\n",
            "step  877, loss: 3.202576 | lr: 2.4000e-04 | norm: 2.0590 dt: 532.81ms, tok/sec: 15375.00\n",
            "step  878, loss: 3.153653 | lr: 2.4000e-04 | norm: 1.8717 dt: 538.26ms, tok/sec: 15219.47\n",
            "step  879, loss: 2.958102 | lr: 2.4000e-04 | norm: 1.9139 dt: 532.88ms, tok/sec: 15372.96\n",
            "step  880, loss: 3.187789 | lr: 2.4000e-04 | norm: 2.2975 dt: 535.14ms, tok/sec: 15308.02\n",
            "step  881, loss: 3.259775 | lr: 2.4000e-04 | norm: 2.4718 dt: 536.38ms, tok/sec: 15272.80\n",
            "step  882, loss: 3.122483 | lr: 2.4000e-04 | norm: 2.3342 dt: 537.22ms, tok/sec: 15248.80\n",
            "step  883, loss: 3.038535 | lr: 2.4000e-04 | norm: 2.0286 dt: 534.38ms, tok/sec: 15329.80\n",
            "step  884, loss: 3.215721 | lr: 2.4000e-04 | norm: 2.0160 dt: 536.67ms, tok/sec: 15264.41\n",
            "step  885, loss: 3.087990 | lr: 2.4000e-04 | norm: 1.8264 dt: 534.26ms, tok/sec: 15333.32\n",
            "step  886, loss: 3.341607 | lr: 2.4000e-04 | norm: 1.9303 dt: 536.70ms, tok/sec: 15263.63\n",
            "step  887, loss: 3.001752 | lr: 2.4000e-04 | norm: 1.8731 dt: 534.78ms, tok/sec: 15318.48\n",
            "step  888, loss: 3.033924 | lr: 2.4000e-04 | norm: 2.0659 dt: 541.75ms, tok/sec: 15121.30\n",
            "step  889, loss: 3.049726 | lr: 2.4000e-04 | norm: 2.2682 dt: 535.94ms, tok/sec: 15285.16\n",
            "step  890, loss: 3.373709 | lr: 2.4000e-04 | norm: 2.4771 dt: 540.99ms, tok/sec: 15142.60\n",
            "step  891, loss: 3.349566 | lr: 2.4000e-04 | norm: 2.2907 dt: 534.35ms, tok/sec: 15330.89\n",
            "step  892, loss: 3.533048 | lr: 2.4000e-04 | norm: 2.4671 dt: 538.08ms, tok/sec: 15224.60\n",
            "step  893, loss: 3.214432 | lr: 2.4000e-04 | norm: 1.8623 dt: 538.02ms, tok/sec: 15226.30\n",
            "step  894, loss: 3.339233 | lr: 2.4000e-04 | norm: 1.9967 dt: 536.29ms, tok/sec: 15275.33\n",
            "step  895, loss: 3.072759 | lr: 2.4000e-04 | norm: 2.2228 dt: 533.12ms, tok/sec: 15366.11\n",
            "step  896, loss: 3.047109 | lr: 2.4000e-04 | norm: 2.0470 dt: 537.92ms, tok/sec: 15229.03\n",
            "step  897, loss: 3.078950 | lr: 2.4000e-04 | norm: 2.2865 dt: 538.76ms, tok/sec: 15205.19\n",
            "step  898, loss: 2.903478 | lr: 2.4000e-04 | norm: 1.8490 dt: 534.90ms, tok/sec: 15315.06\n",
            "step  899, loss: 3.131198 | lr: 2.4000e-04 | norm: 2.0596 dt: 539.39ms, tok/sec: 15187.42\n",
            "Validation loss: 6.0872\n",
            "step  900, loss: 3.248796 | lr: 2.4000e-04 | norm: 2.3418 dt: 4111.94ms, tok/sec: 1992.25\n",
            "step  901, loss: 3.224970 | lr: 2.4000e-04 | norm: 2.2084 dt: 533.54ms, tok/sec: 15354.10\n",
            "step  902, loss: 2.933853 | lr: 2.4000e-04 | norm: 2.0575 dt: 542.20ms, tok/sec: 15108.92\n",
            "step  903, loss: 2.877875 | lr: 2.4000e-04 | norm: 1.9878 dt: 535.37ms, tok/sec: 15301.45\n",
            "step  904, loss: 2.955105 | lr: 2.4000e-04 | norm: 1.9020 dt: 539.65ms, tok/sec: 15180.32\n",
            "step  905, loss: 3.127962 | lr: 2.4000e-04 | norm: 1.7837 dt: 532.76ms, tok/sec: 15376.50\n",
            "step  906, loss: 2.961134 | lr: 2.4000e-04 | norm: 1.7149 dt: 541.60ms, tok/sec: 15125.59\n",
            "step  907, loss: 2.966400 | lr: 2.4000e-04 | norm: 1.5975 dt: 535.27ms, tok/sec: 15304.28\n",
            "step  908, loss: 2.759606 | lr: 2.4000e-04 | norm: 1.6708 dt: 542.00ms, tok/sec: 15114.48\n",
            "step  909, loss: 2.934180 | lr: 2.4000e-04 | norm: 1.6741 dt: 533.83ms, tok/sec: 15345.68\n",
            "step  910, loss: 2.808775 | lr: 2.4000e-04 | norm: 1.6456 dt: 542.51ms, tok/sec: 15100.23\n",
            "step  911, loss: 3.126421 | lr: 2.4000e-04 | norm: 1.8422 dt: 533.45ms, tok/sec: 15356.55\n",
            "step  912, loss: 3.211228 | lr: 2.4000e-04 | norm: 2.1062 dt: 540.95ms, tok/sec: 15143.75\n",
            "step  913, loss: 3.060651 | lr: 2.4000e-04 | norm: 2.0168 dt: 535.88ms, tok/sec: 15287.05\n",
            "step  914, loss: 3.020976 | lr: 2.4000e-04 | norm: 1.8954 dt: 537.74ms, tok/sec: 15234.01\n",
            "step  915, loss: 2.826398 | lr: 2.4000e-04 | norm: 1.8376 dt: 536.08ms, tok/sec: 15281.29\n",
            "step  916, loss: 3.087415 | lr: 2.4000e-04 | norm: 2.3687 dt: 536.48ms, tok/sec: 15269.85\n",
            "step  917, loss: 3.151200 | lr: 2.4000e-04 | norm: 2.3680 dt: 535.11ms, tok/sec: 15308.96\n",
            "step  918, loss: 3.018310 | lr: 2.4000e-04 | norm: 2.3028 dt: 535.44ms, tok/sec: 15299.56\n",
            "step  919, loss: 2.959285 | lr: 2.4000e-04 | norm: 2.2810 dt: 541.25ms, tok/sec: 15135.26\n",
            "step  920, loss: 3.153146 | lr: 2.4000e-04 | norm: 2.6152 dt: 535.07ms, tok/sec: 15310.20\n",
            "step  921, loss: 3.049559 | lr: 2.4000e-04 | norm: 2.3985 dt: 538.37ms, tok/sec: 15216.38\n",
            "step  922, loss: 3.282911 | lr: 2.4000e-04 | norm: 2.3770 dt: 536.77ms, tok/sec: 15261.74\n",
            "step  923, loss: 2.928013 | lr: 2.4000e-04 | norm: 1.9180 dt: 539.98ms, tok/sec: 15170.98\n",
            "step  924, loss: 2.955876 | lr: 2.4000e-04 | norm: 1.8761 dt: 538.09ms, tok/sec: 15224.25\n",
            "step  925, loss: 2.966883 | lr: 2.4000e-04 | norm: 1.9937 dt: 537.19ms, tok/sec: 15249.67\n",
            "step  926, loss: 3.300979 | lr: 2.4000e-04 | norm: 2.4269 dt: 537.68ms, tok/sec: 15235.78\n",
            "step  927, loss: 3.289169 | lr: 2.4000e-04 | norm: 2.4927 dt: 535.72ms, tok/sec: 15291.61\n",
            "step  928, loss: 3.504910 | lr: 2.4000e-04 | norm: 3.2466 dt: 541.18ms, tok/sec: 15137.18\n",
            "step  929, loss: 3.175946 | lr: 2.4000e-04 | norm: 2.8995 dt: 538.15ms, tok/sec: 15222.53\n",
            "step  930, loss: 3.273893 | lr: 2.4000e-04 | norm: 2.5424 dt: 536.58ms, tok/sec: 15267.17\n",
            "step  931, loss: 3.004093 | lr: 2.4000e-04 | norm: 2.3059 dt: 534.59ms, tok/sec: 15323.93\n",
            "step  932, loss: 2.970308 | lr: 2.4000e-04 | norm: 1.9792 dt: 539.79ms, tok/sec: 15176.29\n",
            "step  933, loss: 2.977579 | lr: 2.4000e-04 | norm: 2.0494 dt: 534.46ms, tok/sec: 15327.52\n",
            "step  934, loss: 2.807481 | lr: 2.4000e-04 | norm: 1.9539 dt: 540.23ms, tok/sec: 15164.05\n",
            "step  935, loss: 3.023742 | lr: 2.4000e-04 | norm: 2.0324 dt: 534.90ms, tok/sec: 15314.93\n",
            "step  936, loss: 3.153944 | lr: 2.4000e-04 | norm: 2.2303 dt: 541.41ms, tok/sec: 15130.92\n",
            "step  937, loss: 3.136036 | lr: 2.4000e-04 | norm: 2.2719 dt: 536.91ms, tok/sec: 15257.56\n",
            "step  938, loss: 2.854771 | lr: 2.4000e-04 | norm: 2.1143 dt: 537.82ms, tok/sec: 15231.78\n",
            "step  939, loss: 2.799496 | lr: 2.4000e-04 | norm: 1.9632 dt: 532.51ms, tok/sec: 15383.80\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-2ef3e13bffd9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mparam_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m                             )\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    241\u001b[0m             )\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             adamw(\n\u001b[0m\u001b[1;32m    244\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adamw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    876\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_multi_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    697\u001b[0m                 \u001b[0mexp_avg_sq_sqrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_sqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_max_exp_avg_sqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                 \u001b[0mexp_avg_sq_sqrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_sqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_exp_avg_sqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_div_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq_sqrt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "config = Config(vocab_size=50304)\n",
        "\n",
        "model = GPT(Config())\n",
        "model.to(device)\n",
        "model = torch.compile(model)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(), lr=3e-4, betas=[0.9, 0.95], weight_decay=0.1\n",
        ")\n",
        "max_steps = 2500\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_steps_recorded = []\n",
        "\n",
        "for step in range(max_steps):\n",
        "    t0 = time.time()\n",
        "\n",
        "    # model evaluation\n",
        "    if step % 100 == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss_accum = 0.0\n",
        "            val_loss_steps = 20\n",
        "            for _ in range(val_loss_steps):\n",
        "                x, y = val_loader.next_batch()\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                logits, loss = model(x, y)\n",
        "\n",
        "                loss = loss / val_loss_steps\n",
        "                val_loss_accum += loss.detach()\n",
        "        val_loss_value = val_loss_accum.item()\n",
        "        print(f\"Validation loss: {val_loss_value:.4f}\")\n",
        "        val_losses.append(val_loss_value)\n",
        "        val_steps_recorded.append(step)\n",
        "\n",
        "    # if step > 0 and step % 100 == 0:\n",
        "    #   model.eval()\n",
        "    #   num_return_sequences = 4\n",
        "    #   max_length = 10\n",
        "    #   tokens = enc.encode(\"We are accounted poor citizens\")\n",
        "    #   tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "    #   # Add batch dimension: [length] -> [num_return_sequences, length]\n",
        "    #   xgen = tokens.repeat(num_return_sequences, 1)  # Shape: [4, length]\n",
        "    #   xgen = xgen.to(device)  # Move to device\n",
        "\n",
        "    #   sample_rng = torch.Generator(device=device)\n",
        "    #   sample_rng.manual_seed(42)\n",
        "\n",
        "    #   while xgen.size(1) < max_length:\n",
        "    #       with torch.no_grad():\n",
        "    #           logits, _ = model(xgen)  # Shape: [num_return_sequences, sequence_length, vocab_size]\n",
        "\n",
        "    #           logits = logits[:, -1, :]  # Get logits for the last token: [num_return_sequences, vocab_size]\n",
        "    #           probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "    #           topk_probs, topk_indices = torch.topk(probs, 30, dim=-1)  # [num_return_sequences, 30]\n",
        "    #           ix = torch.multinomial(topk_probs, 1, generator=sample_rng)  # [num_return_sequences, 1]\n",
        "    #           xcol = torch.gather(topk_indices, -1, ix)  # [num_return_sequences, 1]\n",
        "    #           xgen = torch.cat((xgen, xcol), dim=1)  # Append new tokens: [num_return_sequences, sequence_length+1]\n",
        "\n",
        "    #   for i in range(num_return_sequences):\n",
        "    #       tokens = xgen[i, :max_length].tolist()  # Fix: Correct variable name (tokens - xgen was a typo)\n",
        "    #       decoded = enc.decode(tokens)\n",
        "    #       print(f\"sample {i}: {decoded}\")\n",
        "\n",
        "    # model training\n",
        "    model.train()\n",
        "    loss_accum = 0.0\n",
        "    optimizer.zero_grad()\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "        x, y = train_loader.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits, loss = model(x, y)\n",
        "\n",
        "        loss = loss / grad_accum_steps\n",
        "        loss_accum += loss.detach()\n",
        "        loss.backward()\n",
        "    # gradient clipping -> as per deepseek-v2 paper\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    lr = get_lr(step)  # learning rate scheduler\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "    optimizer.step()\n",
        "    torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0  # time difference in seconds\n",
        "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps\n",
        "    tokens_per_sec = tokens_processed / dt\n",
        "\n",
        "    train_loss_value = loss_accum.item()\n",
        "    train_losses.append(train_loss_value)\n",
        "    print(\n",
        "        f\"step {step:4d}, loss: {loss_accum.item():.6f} | lr: {lr:.4e} | norm: {norm:.4f} dt: {dt*1000:.2f}ms, tok/sec: {tokens_per_sec:.2f}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "YV3o24YINXvZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "a64156cc-b085-4f91-dc01-eb334d617bf7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "# Plot training loss (use the actual number of steps completed)\n",
        "plt.plot(range(len(train_losses)), train_losses, label=\"Training Loss\", color=\"blue\")\n",
        "# Plot validation loss (recorded every 100 steps)\n",
        "plt.plot(val_steps_recorded, val_losses, label=\"Validation Loss\", color=\"orange\", marker=\"o\")\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss Over Time\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}